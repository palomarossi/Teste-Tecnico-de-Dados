{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL2nTiEQ4YCg"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Isto está formatado como código\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye9hrQA5DAeC"
   },
   "source": [
    "##Inserindo a biblioteca Kaggle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbLj4ZicCwq1",
    "outputId": "2bf43663-8b59-4fd7-ff34-a81fabecdc8b"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-V8N_hBDHh_"
   },
   "source": [
    "##Configurando a API do Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7M8OiQYC8Sz",
    "outputId": "34fa1520-849e-4261-d539-9ca08048eeab"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Crie o conteúdo do kaggle.json\n",
    "kaggle_json_content = \"\"\"\n",
    "{\n",
    "  \"username\": \"palomaaugustarossi\",\n",
    "  \"key\": \"9cf4bdd7a37681b50ed2306db9ebe62\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Salve o arquivo temporariamente\n",
    "with open('kaggle.json', 'w') as f:\n",
    "    f.write(kaggle_json_content)\n",
    "\n",
    "# Mova para o local correto e ajuste as permissões\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Verifique se está correto\n",
    "!cat ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT_KvCJOFXBL"
   },
   "source": [
    "##Baixando o Dataset e Instalar Dependência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f1d9t6MFW1_",
    "outputId": "92d11443-1afd-481c-9178-657133e38344"
   },
   "outputs": [],
   "source": [
    "# Baixar o dataset\n",
    "!kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "\n",
    "# Descompactar os arquivos\n",
    "!unzip -q brazilian-ecommerce.zip -d data\n",
    "\n",
    "# Listar arquivos baixados\n",
    "!ls data\n",
    "\n",
    "#Instalar PandaSQL\n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi0Hi9Ed8ybp"
   },
   "source": [
    "##Configurações do ambiente\n",
    "\n",
    "Inserção das bibiotecas e padrões de estilo e medidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NapMs3888cu",
    "outputId": "c67b4518-9f0e-4ac9-e225-39ea5027371e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.ticker as mtick\n",
    "from datetime import datetime\n",
    "\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output as clear_display_output\n",
    "\n",
    "from pandasql import sqldf\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def pysqldf(q):\n",
    "    return sqldf(q, globals())\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Executado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCH0qQa7GWlv"
   },
   "source": [
    "##Carregar datasets\n",
    "Forem inseridos os datasets, carregados e no print conseguimos ter uma visão geral de cada um deles atravez das colunas e primeiras linhas de cada um.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad2da1oo8tUS",
    "outputId": "cffca50d-af89-48b9-a15b-ba9d38b5dc81"
   },
   "outputs": [],
   "source": [
    "def carregar_datasets():\n",
    "    print(\"Carregando datasets da pasta 'data/'...\")\n",
    "    datasets_local = {\n",
    "        \"customers\": pd.read_csv(\"data/olist_customers_dataset.csv\"),\n",
    "        \"geolocation\": pd.read_csv(\"data/olist_geolocation_dataset.csv\"),\n",
    "        \"order_items\": pd.read_csv(\"data/olist_order_items_dataset.csv\"),\n",
    "        \"order_payments\": pd.read_csv(\"data/olist_order_payments_dataset.csv\"),\n",
    "        \"order_reviews\": pd.read_csv(\"data/olist_order_reviews_dataset.csv\"),\n",
    "        \"orders\": pd.read_csv(\"data/olist_orders_dataset.csv\"),\n",
    "        \"products\": pd.read_csv(\"data/olist_products_dataset.csv\"),\n",
    "        \"sellers\": pd.read_csv(\"data/olist_sellers_dataset.csv\"),\n",
    "        \"product_category_name_translation\": pd.read_csv(\"data/product_category_name_translation.csv\")\n",
    "    }\n",
    "    for name, df in datasets_local.items():\n",
    "        globals()[name] = df\n",
    "    print(\"Datasets carregados e disponíveis globalmente para pandasql.\")\n",
    "\n",
    "    print(\"\\n--- Amostra dos Datasets Carregados (Primeiras 5 Linhas) ---\")\n",
    "    for name, df in datasets_local.items():\n",
    "        print(f\"\\nDataset: {name}\")\n",
    "        print(df.head())\n",
    "        print(\"--------------------\")\n",
    "\n",
    "# Chamar a função para carregar os datasets\n",
    "carregar_datasets()\n",
    "\n",
    "print(\"\\nExecutado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AZQXzslJ2Eq"
   },
   "source": [
    "# **Limpeza dos dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9K0FlNU-U6z"
   },
   "source": [
    "##Tratamento dos Nulos\n",
    "Foi identificado a necessidade de tratamento de nulos nos datasets orders, products e order_reviews.  \n",
    "\n",
    "* **Orders** foca no custumer_id e depois atualiza.\n",
    "* **Products** fizemos o tratamento substituindo para 'outros' os produtos que estavam sem a categoria. Dimensões e peso nulos foram preenchidos com a média da categoria ou média geral\n",
    "* **Order_reviews** os campos nulos removidos causariam grande impacto na analise pois interlacalam entre titulo do comentario e mensagem do comentario. Os campos, quando nulos, são preenchidos por 'sem titulo' e 'sem comentário'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTTBTyIS-W31",
    "outputId": "2dd7dd95-9a20-4a1d-b6b1-bd5dfc5050c6"
   },
   "outputs": [],
   "source": [
    "def tratar_valores_nulos():\n",
    "    print(\"Iniciando tratamento de valores nulos...\")\n",
    "\n",
    "    # orders\n",
    "    print(\"Tratando nulos em 'orders'...\")\n",
    "    orders_df = globals()['orders'] # Acessa o DataFrame global\n",
    "    orders_df.dropna(subset=['customer_id'], inplace=True)\n",
    "    globals()['orders'] = orders_df # Garante que o global seja atualizado\n",
    "    print(\"'orders' atualizado.\")\n",
    "\n",
    "    # products\n",
    "    print(\"Tratando nulos em 'products'...\")\n",
    "    products_df = globals()['products']\n",
    "    products_df['product_category_name'] = products_df['product_category_name'].fillna('outros')\n",
    "    for col_dim in ['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']:\n",
    "        if products_df[col_dim].isnull().any():\n",
    "            # Tenta preencher com a média da categoria primeiro\n",
    "            products_df[col_dim] = products_df.groupby('product_category_name')[col_dim].transform(lambda x: x.fillna(x.mean()))\n",
    "            # Se ainda houver nulos (ex: categoria nova ou todos nulos na categoria), preenche com a média geral\n",
    "            if products_df[col_dim].isnull().any():\n",
    "                 products_df[col_dim] = products_df[col_dim].fillna(products_df[col_dim].mean())\n",
    "    globals()['products'] = products_df\n",
    "    print(\"'products' atualizado.\")\n",
    "\n",
    "    # order_reviews\n",
    "    print(\"Tratando nulos em 'order_reviews'...\")\n",
    "    order_reviews_df = globals()['order_reviews']\n",
    "    order_reviews_df['review_comment_title'] = order_reviews_df['review_comment_title'].fillna('sem_titulo')\n",
    "    order_reviews_df['review_comment_message'] = order_reviews_df['review_comment_message'].fillna('sem_comentario')\n",
    "    globals()['order_reviews'] = order_reviews_df\n",
    "    print(\"'order_reviews' atualizado.\")\n",
    "\n",
    "    print(\"Tratamento de valores nulos concluído.\")\n",
    "\n",
    "# Chamar a função para tratar valores nulos\n",
    "tratar_valores_nulos()\n",
    "\n",
    "# Opcional: Verificar amostras\n",
    "print(\"\\n--- Amostra de DataFrames Após Tratamento de Nulos (Primeiras 3 Linhas) ---\")\n",
    "if 'orders' in globals():\n",
    "    print(\"\\nDataset: orders (após nulos)\")\n",
    "    print(globals()['orders'].head(3))\n",
    "    print(\"--------------------\")\n",
    "if 'products' in globals():\n",
    "    print(\"\\nDataset: products (após nulos)\")\n",
    "    print(globals()['products'].head(3))\n",
    "    print(\"--------------------\")\n",
    "\n",
    "print(\"\\nExecutado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8m48zUZEqkC"
   },
   "source": [
    "##Remoção de duplicatas.\n",
    "\n",
    "*   **Customers, sellers, products:** Duplicatas baseadas em seus respectivos IDs foram removidas. O registro original foi mantido e as demais entradas repetudas descartadase descartadas, como atualizações incorretas ou dados redundantes.\n",
    "*   **order_items:** Esta aplicação remove registros duplicados da tabela de itens de pedido (order_items), usando uma chave composta com os campos: ID do pedido, ID do produto e ID do item no pedido. Quando existem linhas repetidas com esses mesmos valores, apenas a primeira ocorrência é mantida, e as demais são excluídas. Isso evita dados redundantes e garante que a base esteja limpa e consistente para análises, relatórios ou integrações com outros sistemas.\n",
    "* **Geolocation:** Linhas inteiramente duplicadas foram removidas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8smRftsiKHc6",
    "outputId": "7476a4c8-29f8-4ba9-c585-3153288dfcff"
   },
   "outputs": [],
   "source": [
    "#Preparação dos Dados - Remoção de Duplicatas\n",
    "print(\"--- Preparação dos Dados - Remoção de Duplicatas ---\")\n",
    "\n",
    "# Remover duplicatas de 'customers'\n",
    "print(f\"Shape original de customers: {customers.shape}\")\n",
    "customers.drop_duplicates(subset=['customer_id'], keep='first', inplace=True)\n",
    "print(f\"Shape de customers após remover duplicatas: {customers.shape}\")\n",
    "\n",
    "# Remover duplicatas de 'sellers'\n",
    "print(f\"\\nShape original de sellers: {sellers.shape}\")\n",
    "sellers.drop_duplicates(subset=['seller_id'], keep='first', inplace=True)\n",
    "print(f\"Shape de sellers após remover duplicatas: {sellers.shape}\")\n",
    "\n",
    "# Remover duplicatas de 'products'\n",
    "# (products já foi modificado na seção de nulos, usamos a versão atual)\n",
    "print(f\"\\nShape original de products: {products.shape}\")\n",
    "products.drop_duplicates(subset=['product_id'], keep='first', inplace=True)\n",
    "print(f\"Shape de products após remover duplicatas: {products.shape}\")\n",
    "\n",
    "# Remover duplicatas de 'order_items'\n",
    "# (order_items já foi modificado na seção de conversão de tipos, usamos a versão atual)\n",
    "print(f\"\\nShape original de order_items: {order_items.shape}\")\n",
    "order_items.drop_duplicates(subset=['order_id', 'product_id', 'order_item_id'], keep='first', inplace=True)\n",
    "print(f\"Shape de order_items após remover duplicatas: {order_items.shape}\")\n",
    "\n",
    "# Remover duplicatas de 'geolocation'\n",
    "print(f\"\\nShape original de geolocation: {geolocation.shape}\")\n",
    "geolocation.drop_duplicates(inplace=True)\n",
    "print(f\"Shape de geolocation após remover duplicatas: {geolocation.shape}\")\n",
    "\n",
    "print(\"\\nRemoção de duplicatas concluída.\")\n",
    "print(\"-----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGDbJdST-r35"
   },
   "source": [
    "##Conversão dos Tipos de Dados\n",
    "Nesta etapa, padronizamos os formatos de data nos datasets orders e order_items para viabilizar análises temporais precisas (como cálculo de prazos de entrega) e integração com outras bases. A função 'converter_tipos_dados' realiza as seguintes ações:\n",
    "\n",
    "Conversão para Datetime:\n",
    "\n",
    "*  No **dataset orders**, colunas como order_purchase_timestamp e\n",
    "order_delivered_customer_date são convertidas para o tipo datetime\n",
    "*  Em **order_items**, a coluna shipping_limit_date também é padronizada.\n",
    "\n",
    "\n",
    "Verificação de Integridade: Exibe os tipos de dados antes e depois da conversão para garantir a correta transformação. Mostra uma amostra das primeiras linhas e o shape dos datasets após a alteração. Esta padronização é essencial para garantir consistência nas análises subsequentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xHvfDPk-uKC",
    "outputId": "b0955100-69a7-4430-de2e-44aeb83d6ea2"
   },
   "outputs": [],
   "source": [
    "def converter_tipos_dados():\n",
    "    print(\"Iniciando conversão de tipos de dados...\")\n",
    "\n",
    "    # orders DataFrame\n",
    "    print(\"\\nConvertendo colunas de data em 'orders'...\")\n",
    "    orders_df = globals()['orders']\n",
    "    date_cols_orders = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "\n",
    "    print(\"Tipos de dados ANTES da conversão em 'orders':\")\n",
    "    print(orders_df[date_cols_orders].dtypes)\n",
    "\n",
    "    for col in date_cols_orders:\n",
    "        orders_df[col] = pd.to_datetime(orders_df[col], errors='coerce')\n",
    "\n",
    "    globals()['orders'] = orders_df\n",
    "    print(\"\\nTipos de dados DEPOIS da conversão em 'orders':\")\n",
    "    print(orders_df[date_cols_orders].dtypes)\n",
    "    print(f\"'orders' atualizado. Shape: {orders_df.shape}\")\n",
    "\n",
    "    # order_items DataFrame\n",
    "    print(\"\\nConvertendo colunas de data em 'order_items'...\")\n",
    "    order_items_df = globals()['order_items']\n",
    "    date_cols_order_items = ['shipping_limit_date']\n",
    "\n",
    "    print(\"Tipos de dados ANTES da conversão em 'order_items':\")\n",
    "    print(order_items_df[date_cols_order_items].dtypes)\n",
    "\n",
    "    for col in date_cols_order_items:\n",
    "        order_items_df[col] = pd.to_datetime(order_items_df[col], errors='coerce')\n",
    "\n",
    "    globals()['order_items'] = order_items_df\n",
    "    print(\"\\nTipos de dados DEPOIS da conversão em 'order_items':\")\n",
    "    print(order_items_df[date_cols_order_items].dtypes)\n",
    "    print(f\"'order_items' atualizado. Shape: {order_items_df.shape}\")\n",
    "\n",
    "    print(\"\\nConversão de tipos de dados concluída.\")\n",
    "\n",
    "# Chamar a função para converter os tipos de dados\n",
    "converter_tipos_dados()\n",
    "\n",
    "# Opcional: Verificar as primeiras linhas e dtypes para confirmar as mudanças\n",
    "print(\"\\n--- Amostra de DataFrames Após Conversão de Tipos (Primeiras 3 Linhas) ---\")\n",
    "if 'orders' in globals():\n",
    "    print(\"\\nDataset: orders (após conversão de tipos)\")\n",
    "    print(globals()['orders'][['order_id', 'order_purchase_timestamp', 'order_approved_at']].head(3))\n",
    "    print(f\"Shape: {globals()['orders'].shape}\")\n",
    "    print(\"--------------------\")\n",
    "if 'order_items' in globals():\n",
    "    print(\"\\nDataset: order_items (após conversão de tipos)\")\n",
    "    print(globals()['order_items'][['order_id', 'order_item_id', 'shipping_limit_date']].head(3))\n",
    "    print(f\"Shape: {globals()['order_items'].shape}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "print(\"\\nExecutado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9B-Dq55_P13"
   },
   "source": [
    "##Normalização das colunas\n",
    "Nomes de categorias em 'products' e 'product_category_name_translation' foram convertidos para minúsculas, underscores substituídos por espaços e espaços extras removidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iM32Agii_RUL",
    "outputId": "9a4410d7-568f-40e8-c8ce-e9f21e55bbce"
   },
   "outputs": [],
   "source": [
    "# Preparação dos Dados - Normalização e Transformações Adicionais\n",
    "\n",
    "# Normalização de colunas de texto\n",
    "print(\"Normalizando colunas de texto...\")\n",
    "\n",
    "# Normalizar 'product_category_name' no DataFrame 'products'\n",
    "if 'products' in globals():\n",
    "    products_df = globals()['products']\n",
    "    if 'product_category_name' in products_df.columns:\n",
    "        products_df['product_category_name'] = products_df['product_category_name'].str.lower().str.replace('_', ' ', regex=False).str.strip()\n",
    "        globals()['products'] = products_df\n",
    "        print(\"Coluna 'product_category_name' em 'products' normalizada.\")\n",
    "        print(\"Exemplo após normalização em 'products':\")\n",
    "        print(products_df[['product_category_name']].head())\n",
    "    else:\n",
    "        print(\"Coluna 'product_category_name' não encontrada em 'products'.\")\n",
    "else:\n",
    "    print(\"DataFrame 'products' não encontrado no escopo global.\")\n",
    "\n",
    "# Normalizar 'product_category_name' no DataFrame 'product_category_name_translation'\n",
    "if 'product_category_name_translation' in globals():\n",
    "    translation_df = globals()['product_category_name_translation']\n",
    "    if 'product_category_name' in translation_df.columns:\n",
    "        translation_df['product_category_name'] = translation_df['product_category_name'].str.lower().str.replace('_', ' ', regex=False).str.strip()\n",
    "        globals()['product_category_name_translation'] = translation_df\n",
    "        print(\"\\nColuna 'product_category_name' em 'product_category_name_translation' normalizada.\")\n",
    "        print(\"Exemplo após normalização em 'product_category_name_translation':\")\n",
    "        print(translation_df[['product_category_name', 'product_category_name_english']].head())\n",
    "    else:\n",
    "        print(\"Coluna 'product_category_name' não encontrada em 'product_category_name_translation'.\")\n",
    "else:\n",
    "    print(\"DataFrame 'product_category_name_translation' não encontrado no escopo global.\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HglQ86fL_WzX"
   },
   "source": [
    "# Média da Geolocalização\n",
    "Este código realiza a consolidação de dados geográficos, agregando as coordenadas por CEP para otimizar análises espaciais e integrações com outros datasets. O processo cria um novo DataFrame com as médias de latitude e longitude por prefixo de CEP, impactando diretamente a qualidade e eficiência das análises de distribuição geográfica.\n",
    "\n",
    "Agregação de Coordenadas:\n",
    "* Agrupa os dados do DataFrame geolocation por geolocation_zip_code_prefix\n",
    "* Calcula a média das latitudes (geolocation_lat) e longitudes (geolocation_lng) para cada CEP\n",
    "\n",
    "Verificações:\n",
    "*   Confirma a existência do DataFrame geolocation\n",
    "*   Valida a presença das colunas necessárias\n",
    "*   Verifica se o DataFrame não está vazio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eS1dqGt_Yqf",
    "outputId": "70787786-830c-4549-99aa-07d8d152fee8"
   },
   "outputs": [],
   "source": [
    "# Agregação de Geolocalização\n",
    "print(\"Agregando dados de geolocalização...\")\n",
    "\n",
    "if 'geolocation' in globals():\n",
    "    geolocation_df = globals()['geolocation']\n",
    "\n",
    "    # Verificar se o DataFrame não está vazio e se as colunas necessárias existem\n",
    "    if not geolocation_df.empty and all(col in geolocation_df.columns for col in ['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']):\n",
    "        geolocation_agg_df = geolocation_df.groupby('geolocation_zip_code_prefix').agg(\n",
    "            geolocation_lat=('geolocation_lat', 'mean'),\n",
    "            geolocation_lng=('geolocation_lng', 'mean')\n",
    "        ).reset_index()\n",
    "\n",
    "        globals()['geolocation_agg'] = geolocation_agg_df\n",
    "        print(\"Agregação de geolocalização concluída. DataFrame 'geolocation_agg' criado.\")\n",
    "        print(\"Shape de 'geolocation_agg':\", geolocation_agg_df.shape)\n",
    "        print(\"Exemplo de 'geolocation_agg':\")\n",
    "        print(geolocation_agg_df.head())\n",
    "    else:\n",
    "        print(\"DataFrame 'geolocation' está vazio ou não contém as colunas necessárias para agregação.\")\n",
    "else:\n",
    "    print(\"DataFrame 'geolocation' não encontrado no escopo global.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-A5Z23I_gNN"
   },
   "source": [
    "##Merge para Tradução das Categorias\n",
    "Este código realiza a integração entre os DataFrames products e product_category_name_translation para criar uma coluna unificada de categorias de produtos (product_category_name_final), combinando os nomes originais com suas traduções em inglês quando disponíveis. Essa padronização gera impactos imediatos na qualidade dos dados, permitindo análises mais consistentes e integrações precisas com outros datasets.\n",
    "\n",
    "Processo de Junção:\n",
    "\n",
    "* Verifica a existência dos DataFrames products e product_category_name_translation\n",
    "* Confirma a presença da coluna chave product_category_name em ambos\n",
    "* Executa um merge left para preservar todos os produtos, mesmo sem tradução\n",
    "\n",
    "Criação da Coluna Unificada:\n",
    "\n",
    "* Prioriza a versão em inglês (product_category_name_english)\n",
    "* Mantém o nome original quando a tradução não existe\n",
    "* Atualiza globalmente o DataFrame products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Z37p0KX_iMB",
    "outputId": "7bb4c23b-ac91-42e0-f88b-00f07cdc074d"
   },
   "outputs": [],
   "source": [
    "# Merge para tradução de categorias de produtos\n",
    "print(\"Realizando merge da tradução de categorias de produtos...\")\n",
    "\n",
    "if 'products' in globals() and 'product_category_name_translation' in globals():\n",
    "    products_df = globals()['products']\n",
    "    translation_df = globals()['product_category_name_translation']\n",
    "\n",
    "    # Verificar se as colunas de merge existem\n",
    "    if 'product_category_name' in products_df.columns and 'product_category_name' in translation_df.columns:\n",
    "        products_merged_df = pd.merge(\n",
    "            products_df,\n",
    "            translation_df,\n",
    "            on='product_category_name',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Criar 'product_category_name_final'\n",
    "        # Usar 'product_category_name_english' se disponível, senão usar 'product_category_name' original\n",
    "        products_merged_df['product_category_name_final'] = products_merged_df['product_category_name_english'].fillna(products_merged_df['product_category_name'])\n",
    "\n",
    "        globals()['products'] = products_merged_df # Atualizar o DataFrame global 'products'\n",
    "\n",
    "        print(\"Merge da tradução de categorias de produtos concluído.\")\n",
    "        print(\"DataFrame 'products' atualizado com 'product_category_name_final'.\")\n",
    "        print(\"Shape de 'products' após merge:\", products_merged_df.shape)\n",
    "        print(\"Exemplo de 'products' com a nova coluna (verifique 'product_category_name', 'product_category_name_english', 'product_category_name_final'):\")\n",
    "        print(products_merged_df[['product_category_name', 'product_category_name_english', 'product_category_name_final']].head())\n",
    "    else:\n",
    "        print(\"Coluna 'product_category_name' não encontrada em 'products' ou 'product_category_name_translation' para o merge.\")\n",
    "else:\n",
    "    print(\"DataFrames 'products' ou 'product_category_name_translation' não encontrados no escopo global.\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "# Sumário da Preparação dos Dados\n",
    "print(\"\\n--- Descrição dos Passos de Preparação dos Dados ---\")\n",
    "print(\"1. Carregamento: Todos os arquivos CSV foram carregados em DataFrames pandas e disponibilizados globalmente (ajustado para caminhos 'data/').\")\n",
    "print(\"2. Conversão de Datas: Colunas de timestamp foram convertidas para o tipo datetime (com errors='coerce').\")\n",
    "print(\"3. Tratamento de Nulos:\")\n",
    "print(\"   - orders: Linhas com 'customer_id' nulo foram removidas.\")\n",
    "print(\"   - products: 'product_category_name' nulo foi preenchido com 'outros'.\")\n",
    "print(\"   - products: Dimensões e peso nulos foram preenchidos com a média da categoria ou média geral.\")\n",
    "print(\"   - order_reviews: 'review_comment_title' nulo preenchido com 'sem_titulo', 'review_comment_message' nulo preenchido com 'sem_comentario'.\")\n",
    "print(\"4. Remoção de Duplicatas:\")\n",
    "print(\"   - customers, sellers, products: Duplicatas baseadas em seus respectivos IDs foram removidas (keep='first').\")\n",
    "print(\"   - order_items: Duplicatas baseadas na chave composta (order_id, product_id, order_item_id) foram removidas (keep='first').\")\n",
    "print(\"   - geolocation: Linhas inteiramente duplicadas foram removidas.\")\n",
    "print(\"5. Normalização de Colunas de Texto:\")\n",
    "print(\"   - Nomes de categorias em 'products' e 'product_category_name_translation' foram convertidos para minúsculas, underscores substituídos por espaços e espaços extras removidos.\")\n",
    "print(\"6. Agregação de Geolocalização:\")\n",
    "print(\"   - Criado DataFrame 'geolocation_agg' com a média de latitude/longitude por 'geolocation_zip_code_prefix', disponível globalmente.\")\n",
    "print(\"7. Tradução de Categorias:\")\n",
    "print(\"   - DataFrame 'products' atualizado com nomes de categorias em inglês (coluna 'product_category_name_final'), usando a tradução ou o nome original.\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Limpeza e preparação dos dados concluídas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFElfuqKAZiq"
   },
   "source": [
    "##Modelo Relacional e Cálculo Logístico\n",
    "Primeiro, estabelece um modelo relacional completo, conectando os principais DataFrames do sistema através de chaves primárias e estrangeiras - como orders (pedidos) com customers (clientes), order_items (itens), order_payments (pagamentos) e order_reviews (avaliações), além de integrar informações de produtos, vendedores e geolocalização.\n",
    "\n",
    "Segundo, desenvolveremos uma função Haversine robusta para cálculo preciso de distâncias geográficas, que inclui: conversão de coordenadas para radianos, aplicação da fórmula matemática considerando a curvatura da Terra, tratamento de valores inválidos (NaN) e um sistema sofisticado de arredondamento que analisa as terceira e quarta casas decimais para decidir sobre o arredondamento final.\n",
    "\n",
    "Esta implementação permite não apenas análises relacionais completas entre todos os aspectos do negócio, mas também viabiliza cálculos logísticos precisos, como distâncias entre clientes e vendedores, otimização de rotas e análises de proximidade, fundamentais para estratégias de entrega e expansão territorial do e-commerce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "2JVQhMByAcwb",
    "outputId": "9f279ab5-8b01-4bcb-cfab-fb855b1cdc7f"
   },
   "outputs": [],
   "source": [
    "#  Modelo Relacional e Funções Auxiliares\n",
    "\n",
    "# O modelo relacional é implementado através de DataFrames e suas chaves.\n",
    "# As conexões principais são:\n",
    "# - orders.customer_id -> customers.customer_id\n",
    "# - orders.order_id -> order_items.order_id\n",
    "# - orders.order_id -> order_payments.order_id\n",
    "# - orders.order_id -> order_reviews.order_id\n",
    "# - order_items.product_id -> products.product_id\n",
    "# - order_items.seller_id -> sellers.seller_id\n",
    "# - customers.customer_zip_code_prefix -> geolocation_agg.geolocation_zip_code_prefix (para geolocalização do cliente)\n",
    "# - sellers.seller_zip_code_prefix -> geolocation_agg.geolocation_zip_code_prefix (para geolocalização do vendedor)\n",
    "# - products.product_category_name -> product_category_name_translation.product_category_name (para tradução)\n",
    "\n",
    "# Cria o grafo\n",
    "dot = Digraph(comment='Modelo Relacional Olist', format='png')\n",
    "dot.attr(rankdir='LR', bgcolor='white')\n",
    "\n",
    "# Estilo padrão das tabelas\n",
    "table_style = {\n",
    "    'shape': 'plaintext',\n",
    "    'fontname': 'monospace',\n",
    "}\n",
    "\n",
    "# Define as tabelas (com PK e FK)\n",
    "tabelas = {\n",
    "    'orders': ['order_id (PK)', 'customer_id (FK)'],\n",
    "    'customers': ['customer_id (PK)', 'zip_code_prefix (FK)'],\n",
    "    'order_items': ['order_id (FK)', 'product_id (FK)', 'seller_id (FK)'],\n",
    "    'order_payments': ['order_id (FK)'],\n",
    "    'order_reviews': ['order_id (FK)'],\n",
    "    'products': ['product_id (PK)', 'product_category_name (FK)'],\n",
    "    'sellers': ['seller_id (PK)', 'zip_code_prefix (FK)'],\n",
    "    'geolocation_agg': ['zip_code_prefix (PK)', 'lat', 'lng'],\n",
    "    'product_category_name_translation': ['product_category_name (PK)', 'product_category_name_english'],\n",
    "}\n",
    "\n",
    "for tabela, colunas in tabelas.items():\n",
    "    tabela_html = f\"\"\"<\n",
    "    <table border=\"1\" cellborder=\"0\" cellspacing=\"0\">\n",
    "        <tr><td bgcolor=\"lightblue\"><b>{tabela}</b></td></tr>\"\"\" + \\\n",
    "        \"\".join([f'<tr><td align=\"left\">{col}</td></tr>' for col in colunas]) + \\\n",
    "        \"</table>>\"\n",
    "    dot.node(tabela, tabela_html, **table_style)\n",
    "\n",
    "# Define as relações (FK → PK)\n",
    "relacoes = [\n",
    "    ('orders', 'customers'),\n",
    "    ('orders', 'order_items'),\n",
    "    ('orders', 'order_payments'),\n",
    "    ('orders', 'order_reviews'),\n",
    "    ('order_items', 'products'),\n",
    "    ('order_items', 'sellers'),\n",
    "    ('customers', 'geolocation_agg'),\n",
    "    ('sellers', 'geolocation_agg'),\n",
    "    ('products', 'product_category_name_translation')\n",
    "]\n",
    "\n",
    "\n",
    "for origem, destino in relacoes:\n",
    "    dot.edge(origem, destino)\n",
    "\n",
    "display(dot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HObljheqn4kI"
   },
   "source": [
    "###Haversine para Cálculo Logístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99z2mcZmn4wX",
    "outputId": "8abd9a6b-40a4-49cc-8246-62693fbb0577"
   },
   "outputs": [],
   "source": [
    "# Função Haversine para calcular distância entre duas coordenadas geográficas\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return np.nan\n",
    "    R = 6371\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    if pd.isna(distance):\n",
    "        return np.nan\n",
    "\n",
    "    dist_times_10000 = distance * 10000\n",
    "\n",
    "    int_dist_times_10000 = int(dist_times_10000)\n",
    "\n",
    "    third_fourth_digits = int_dist_times_10000 % 100\n",
    "\n",
    "    if third_fourth_digits > 50:\n",
    "        rounded_distance = (int_dist_times_10000 // 100 + 1) / 100.0\n",
    "    else:\n",
    "        rounded_distance = (int_dist_times_10000 // 100) / 100.0\n",
    "\n",
    "    return rounded_distance\n",
    "\n",
    "print(\"Função haversine definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evZcGOleAnoU"
   },
   "source": [
    "# **Análise Exploratória de Dados**\n",
    "Nesta seção, vamos explorar os dados para extrair insights e responder as seguintes perguntas de negócio.\n",
    "\n",
    "a) Qual o volume de pedidos por mês? Existe sazonalidade nas vendas?\n",
    "\n",
    "b) Qual a distribuição do tempo de entrega dos pedidos?\n",
    "\n",
    "c) Qual a relação entre o valor do frete e a distância de entrega?\n",
    "\n",
    "d) Quais são as categorias de produtos mais vendidas em termos de faturamento?\n",
    "\n",
    "e) Quais estados brasileiros possuem o maior valor médio de pedido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hREU-pzCAp2j"
   },
   "source": [
    "# Volume de Pedidos por Mês e Sazonalidade\n",
    "Este código realiza uma análise temporal do volume de pedidos em um e-commerce, utilizando o DataFrame orders como base. Primeiro, filtra os registros para garantir que contenham datas válidas de compra (order_purchase_timestamp). Em seguida, cria uma coluna ano_mes para agrupar os pedidos por mês e ano, facilitando a análise mensal. O código agrupa os dados por período (ano_mes) e conta o número de pedidos (order_id), gerando um DataFrame resumido (volume_mensal_py) que exibe o volume de pedidos por mês.\n",
    "\n",
    "# **Insigths**\n",
    "O gráfico demonstra a sazonalidade nas vendas, com um padrão cíclico que se repete ao longo dos meses. **O pico mais pronunciado ocorre em novembro, coincidindo com a Black Friday**, onde o volume de pedidos atinge seu ápice anual, evidenciando o impacto das promoções nesta época do ano.\n",
    "\n",
    "Paralelamente, observa-se uma tendência geral de **crescimento nas vendas ao longo do tempo, sugerindo uma expansão saudável do negócio**, embora com quedas pontuais em meses específicos que podem estar relacionados a fatores sazonais ou econômicos. Os vales entre os picos indicam períodos de menor atividade comercial, que merecem atenção para implementação de estratégias como promoções específicas ou ajustes de estoque.\n",
    "\n",
    "A análise reforça a importância de monitorar esses ciclos para manter a competitividade e aproveitar ao máximo os períodos de alta demanda, como o caso da Black Friday, enquanto se desenvolvem ações para minimizar o impacto dos meses de menor movimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "FFWqI5xaAq2r",
    "outputId": "933181e4-e69a-4e9f-d1d1-c7e975d5001b"
   },
   "outputs": [],
   "source": [
    "df_py = orders.copy()\n",
    "df_py = df_py[df_py['order_purchase_timestamp'].notna()]\n",
    "df_py['ano_mes'] = df_py['order_purchase_timestamp'].dt.to_period('M')\n",
    "volume_mensal_py = df_py.groupby('ano_mes')['order_id'].count().reset_index(name='numero_de_pedidos')\n",
    "volume_mensal_py['ano_mes'] = volume_mensal_py['ano_mes'].astype(str)\n",
    "\n",
    "print(\"\\nVolume de Pedidos por Mês:\")\n",
    "print(volume_mensal_py.head())\n",
    "print(f\"\\nTotal de meses analisados: {len(volume_mensal_py)}\")\n",
    "print(f\"Período coberto: {volume_mensal_py['ano_mes'].min()} a {volume_mensal_py['ano_mes'].max()}\")\n",
    "\n",
    "print(\"\\nAnálise de Sazonalidade:\")\n",
    "print(\"Observando os dados, podemos identificar picos e vales.\")\n",
    "print(\"Um pico notável ocorre geralmente em novembro (potencialmente devido à Black Friday).\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=volume_mensal_py, x='ano_mes', y='numero_de_pedidos')\n",
    "plt.title('Volume de Pedidos por Mês')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaIuZxhtBFmW"
   },
   "source": [
    "##Distribuição do Tempo de Entrega dos Pedidos\n",
    "Este código realiza uma análise detalhada do tempo de entrega dos pedidos em um e-commerce, desde o momento da compra até a entrega ao cliente. O processo inclui:\n",
    "\n",
    "Preparação dos dados:\n",
    "*  Remoção de registros com datas ausentes de compra ou entrega\n",
    "*  Cálculo do tempo de entrega em dias\n",
    "\n",
    "Filtragem e análise:\n",
    "*   Foco em entregas entre 0-100 dias para eliminar outliers extremos\n",
    "*   Geração de estatísticas descritivas (média, mediana, percentis)\n",
    "*   Visualização através de histograma com curva de densidade\n",
    "\n",
    "# **Insights:**\n",
    "\n",
    "A análise revela que, embora a maioria das entregas ocorra dentro do esperado (20-30 dias), existem casos que demandam atenção especial devido a prazos significativamente maiores, sugerindo oportunidades para otimização da cadeia logística.\n",
    "\n",
    "*   Distribuição assimétrica à direita, com maioria das entregas em 20-30 dias\n",
    "*   Presença de uma cauda longa indicando entregas excepcionalmente demoradas\n",
    "*   A cauda longa da distribuição sugere que, embora menos frequentes, alguns pedidos podem levar significativamente mais tempo para serem entregues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TEehKWc_BFX6",
    "outputId": "be95755e-8a13-41fe-d80f-4d9f2db24176"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 5.2 Distribuição do Tempo de Entrega dos Pedidos ---\")\n",
    "\n",
    "df_orders_py = orders.copy()\n",
    "\n",
    "df_orders_py.dropna(subset=['order_purchase_timestamp', 'order_delivered_customer_date'], inplace=True)\n",
    "\n",
    "df_orders_py['tempo_entrega_dias'] = (df_orders_py['order_delivered_customer_date'] - df_orders_py['order_purchase_timestamp']).dt.days\n",
    "\n",
    "df_orders_py_filtrado = df_orders_py[(df_orders_py['tempo_entrega_dias'] >= 0) & (df_orders_py['tempo_entrega_dias'] <= 100)].copy() # Added .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "print(\"\\nDistribuição do Tempo de Entrega (em dias):\")\n",
    "print(df_orders_py_filtrado[['order_id', 'tempo_entrega_dias']].head())\n",
    "\n",
    "print(f\"\\nDescrição do Tempo de Entrega (dias):\\n{df_orders_py_filtrado['tempo_entrega_dias'].describe()}\")\n",
    "\n",
    "print(\"\\nVisualizando a Distribuição do Tempo de Entrega:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_orders_py_filtrado['tempo_entrega_dias'], bins=50, kde=True)\n",
    "plt.title('Distribuição do Tempo de Entrega dos Pedidos (0-100 dias)')\n",
    "plt.xlabel('Tempo de Entrega (dias)')\n",
    "plt.ylabel('Número de Pedidos')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnálise da Distribuição do Tempo de Entrega:\")\n",
    "print(\"A distribuição é assimétrica à direita, com a maioria das entregas ocorrendo em até ~20-30 dias.\")\n",
    "print(\"O filtro de 0-100 dias foi aplicado para melhor visualização e para remover outliers que poderiam distorcer a análise e o gráfico.\")\n",
    "print(\"A cauda longa da distribuição sugere que, embora menos frequentes, alguns pedidos podem levar significativamente mais tempo para serem entregues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDE_HvyABe-_"
   },
   "source": [
    "##Relação entre o Valor do Frete e a Distância de Entrega.\n",
    "Este código realiza uma análise completa entre o valor do frete e a distância de entrega em um e-commerce. O processo começa com a preparação dos dados, combinando informações de pedidos, clientes, vendedores e geolocalização através de múltiplos merges, tratando valores ausentes e conflitos de colunas. Em seguida, calcula as distâncias entre clientes e vendedores usando a função haversine, filtrando coordenadas inválidas.\n",
    "\n",
    "A análise remove outliers extremos (fretes acima de R$300 e distâncias superiores a 4000 km) e calcula a correlação entre as variáveis. Os resultados são visualizados em um gráfico de dispersão claro, mostrando a relação entre distância e custo do frete.\n",
    "\n",
    "**Tendência:** Se os pontos formam uma nuvem que se inclina para cima da esquerda para a direita, isso reforça a correlação positiva.\n",
    "Dispersão: A dispersão dos pontos indica que outros fatores além da distância também influenciam o frete. Uma grande dispersão significa que, para uma mesma distância, pode haver uma variação considerável no valor do frete. Esses fatores podem incluir peso do produto, dimensões, tipo de serviço de entrega, urgência, etc.\n",
    "\n",
    "**Outliers:** Mesmo após a filtragem, podem existir alguns pontos que se desviam significativamente do padrão geral, representando casos específicos.\n",
    "Com base nos resultados típicos para este dataset, espera-se uma correlação positiva, mas não perfeitamente linear, indicando que a distância é um fator importante, mas não o único, na determinação do preço do frete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "id": "brKJIFxGBfQ0",
    "outputId": "e97d7984-5b54-4ef6-d5b2-79052ada665e"
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Preparando dados para análise de frete vs. distância...\")\n",
    "\n",
    "# Criar cópias para evitar modificar os DataFrames originais\n",
    "df_items = order_items.copy()\n",
    "df_orders = orders.copy()\n",
    "df_customers = customers.copy()\n",
    "df_sellers = sellers.copy()\n",
    "df_geo_agg = geolocation_agg.copy()\n",
    "\n",
    "#Merge para obter dados de cliente e vendedor no mesmo DataFrame\n",
    "data_frete = pd.merge(df_items[['order_id', 'seller_id', 'freight_value']], df_orders[['order_id', 'customer_id']], on='order_id')\n",
    "data_frete = pd.merge(data_frete, df_customers[['customer_id', 'customer_zip_code_prefix']], on='customer_id')\n",
    "data_frete = pd.merge(data_frete, df_sellers[['seller_id', 'seller_zip_code_prefix']], on='seller_id')\n",
    "\n",
    "#Merge com dados de geolocalização para clientes\n",
    "data_frete = pd.merge(data_frete, df_geo_agg, left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left')\n",
    "data_frete.rename(columns={'geolocation_lat': 'customer_lat', 'geolocation_lng': 'customer_lng'}, inplace=True)\n",
    "# Remover a coluna de prefixo de CEP da geolocalização do cliente para evitar conflito no próximo merge\n",
    "if 'geolocation_zip_code_prefix' in data_frete.columns:\n",
    "    data_frete.drop(columns=['geolocation_zip_code_prefix'], inplace=True)\n",
    "\n",
    "#Merge com dados de geolocalização para vendedores\n",
    "data_frete = pd.merge(data_frete, df_geo_agg, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left', suffixes=('_customer', '_seller'))\n",
    "data_frete.rename(columns={'geolocation_lat': 'seller_lat',\n",
    "                           'geolocation_lng': 'seller_lng',\n",
    "                           'geolocation_zip_code_prefix': 'seller_geo_zip_code_prefix'}, inplace=True)\n",
    "\n",
    "if 'geolocation_lat_seller' in data_frete.columns:\n",
    "    data_frete.rename(columns={'geolocation_lat_seller': 'seller_lat',\n",
    "                               'geolocation_lng_seller': 'seller_lng'}, inplace=True)\n",
    "if 'seller_geo_zip_code_prefix' in data_frete.columns:\n",
    "    data_frete.drop(columns=['seller_geo_zip_code_prefix'], inplace=True)\n",
    "\n",
    "\n",
    "#Tratar valores ausentes e calcular distância\n",
    "data_frete.dropna(subset=['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng', 'freight_value'], inplace=True)\n",
    "\n",
    "if data_frete.empty:\n",
    "    print(\"Não foi possível obter dados suficientes (após merges e remoção de nulos) para calcular distâncias.\")\n",
    "else:\n",
    "    print(f\"Calculando distâncias para {len(data_frete)} itens...\")\n",
    "    # Aplicar a função haversine\n",
    "    data_frete['distancia_km'] = data_frete.apply(\n",
    "        lambda row: haversine(row['customer_lat'], row['customer_lng'], row['seller_lat'], row['seller_lng']),\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"Cálculo de distâncias concluído.\")\n",
    "\n",
    "    # Remover NaNs que podem surgir do cálculo de haversine\n",
    "    data_frete.dropna(subset=['distancia_km'], inplace=True)\n",
    "\n",
    "    # Filtrar outliers para melhor visualização e análise de correlação\n",
    "    data_filtrada = data_frete[(data_frete['freight_value'] > 0) & (data_frete['freight_value'] < 300)]\n",
    "    data_filtrada = data_filtrada[(data_filtrada['distancia_km'] > 0) & (data_filtrada['distancia_km'] < 4000)]\n",
    "\n",
    "    if data_filtrada.empty:\n",
    "        print(\"Nenhum dado após filtragem de outliers de frete/distância.\")\n",
    "    else:\n",
    "        print(\"\\nPrimeiras linhas dos dados de Frete e Distância (após tratamento arredondamento para cima quando .005+ ou mantendo o valor quando .004-e filtros):\")\n",
    "        print(data_filtrada[['order_id', 'freight_value', 'distancia_km']].head())\n",
    "\n",
    "        #Calcular correlação\n",
    "        correlacao = data_filtrada['distancia_km'].corr(data_filtrada['freight_value'])\n",
    "        print(f\"\\nCorrelação entre distância (km) e valor do frete (R$): {correlacao:.2f}\")\n",
    "\n",
    "        #Visualizar a relação com um gráfico de dispersão\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='distancia_km', y='freight_value', data=data_filtrada.sample(min(1000, len(data_filtrada))), alpha=0.5) # Usar uma amostra para plots densos\n",
    "        plt.title('Relação entre Distância de Entrega e Valor do Frete')\n",
    "        plt.xlabel('Distância (km)')\n",
    "        plt.ylabel('Valor do Frete (R$)')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhKu-TnqEObQ"
   },
   "source": [
    "**Interpretação dos Resultados (Frete vs. Distância):**\n",
    "\n",
    "A análise da correlação e o gráfico de dispersão nos ajudam a entender a relação entre a distância da entrega e o valor do frete.\n",
    "\n",
    "*   **Correlação**: O valor da correlação (entre -1 e 1) indica a força e a direção da relação linear. Uma correlação positiva próxima de 1 sugere que, à medida que a distância aumenta, o valor do frete também tende a aumentar. Se for próxima de 0, a relação linear é fraca.\n",
    "*   **Gráfico de Dispersão**: O gráfico mostra visualmente essa relação. Podemos observar:\n",
    "    *   **Tendência**: Se os pontos formam uma nuvem que se inclina para cima da esquerda para a direita, isso reforça a correlação positiva.\n",
    "    *   **Dispersão**: A dispersão dos pontos indica que outros fatores além da distância também influenciam o frete. Uma grande dispersão significa que, para uma mesma distância, pode haver uma variação considerável no valor do frete. Esses fatores podem incluir peso do produto, dimensões, tipo de serviço de entrega, urgência, etc.\n",
    "    *   **Outliers**: Mesmo após a filtragem, podem existir alguns pontos que se desviam significativamente do padrão geral, representando casos específicos.\n",
    "\n",
    "Com base nos resultados típicos para este dataset, espera-se uma correlação positiva, mas não perfeitamente linear, indicando que a distância é um fator importante, mas não o único, na determinação do preço do frete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcAwn6dRBftC"
   },
   "source": [
    "##Categorias de Produtos Mais Vendidas (Faturamento)\n",
    "Este código realiza uma análise estratégica das categorias de produtos que mais contribuem para o faturamento de um e-commerce. O processo começa com a combinação dos dados de itens de pedidos com as informações de categorias de produtos, garantindo que apenas categorias válidas sejam consideradas (excluindo valores nulos ou marcados como 'outros')\n",
    "\n",
    "Principais insights gerados:\n",
    "*   As categorias que mais geram mais venda são: health_beauty, watches_gifts e\n",
    "bed_bath_table\n",
    "*   O faturamento de cada uma delas passa de 1 milhão de reais\n",
    "\n",
    "Aplicações práticas:\n",
    "\n",
    "*   Priorização de investimentos em estoque\n",
    "*   Desenvolvimento de campanhas de marketing segmentadas\n",
    "*   Identificação de oportunidades para expansão de catálogo ou remoção de nichos com poucas vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xtJ10XNZBgKA",
    "outputId": "fc2ffce6-8990-413c-f38f-4d047ef62b4f"
   },
   "outputs": [],
   "source": [
    "print(\"--- Categorias de Produtos Mais Vendidas (Faturamento) ---\")\n",
    "\n",
    "#Copiar DataFrames para evitar modificar os originais\n",
    "df_items_eda = order_items.copy()\n",
    "df_products_eda = products.copy()\n",
    "\n",
    "#Merge para obter a categoria do produto para cada item de pedido\n",
    "merged_df_cat_fat = pd.merge(\n",
    "    df_items_eda,\n",
    "    df_products_eda[['product_id', 'product_category_name_final']],\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#Remover linhas onde a categoria do produto é nula ou 'outros'\n",
    "merged_df_cat_fat.dropna(subset=['product_category_name_final'], inplace=True)\n",
    "merged_df_cat_fat = merged_df_cat_fat[merged_df_cat_fat['product_category_name_final'] != 'outros']\n",
    "\n",
    "#Calcular o faturamento total por categoria\n",
    "faturamento_por_categoria = merged_df_cat_fat.groupby('product_category_name_final')['price'].sum().reset_index()\n",
    "\n",
    "#Ordenar por faturamento em ordem decrescente e pegar o top 10\n",
    "top_10_categorias_faturamento = faturamento_por_categoria.sort_values(by='price', ascending=False).head(10)\n",
    "top_10_categorias_faturamento.rename(columns={'product_category_name_final': 'categoria_produto', 'price': 'faturamento_total_R$'}, inplace=True)\n",
    "\n",
    "#Formatar a coluna de faturamento como moeda brasileira\n",
    "top_10_categorias_faturamento_display = top_10_categorias_faturamento.copy()\n",
    "top_10_categorias_faturamento_display['faturamento_total_R$'] = top_10_categorias_faturamento_display['faturamento_total_R$'].map('R$ {:,.2f}'.format)\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Categorias de Produtos por Faturamento:\")\n",
    "print(top_10_categorias_faturamento_display)\n",
    "\n",
    "# Visualização\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x='faturamento_total_R$',\n",
    "    y='categoria_produto',\n",
    "    data=top_10_categorias_faturamento, # Usar o DataFrame para plotagem\n",
    "    hue='categoria_produto',\n",
    "    palette='viridis',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Top 10 Categorias de Produtos por Faturamento', fontsize=16)\n",
    "plt.xlabel('Faturamento Total (R$)', fontsize=14)\n",
    "plt.ylabel('Categoria do Produto', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnálise das Categorias Mais Vendidas por Faturamento:\")\n",
    "print(\"O gráfico e a tabela acima mostram as 10 categorias de produtos que mais geraram receita.\")\n",
    "print(\"Essas informações são cruciais para estratégias de estoque, marketing e desenvolvimento de novos produtos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npJbQ9nUBgj0"
   },
   "source": [
    "##Estados Brasileiros com Maior Valor Médio de Pedido\n",
    "Este código realiza uma análise estratégica para identificar os estados brasileiros com os maiores valores médios de pedido em um e-commerce, combinando dados de pedidos, itens e clientes.\n",
    "\n",
    "# **Insights:**\n",
    "*   Os quatro estados com maior valor médio de compra são Paraíba (PB),  Amapá (AP), o Acre (AC) e a Alagoas (AL).\n",
    "*   Identificação de padrões regionais de consumo. Os estamos com maior valor medio estão no Norte e Nordeste.\n",
    "*   O valor do Top 10 parte de R$172 reais\n",
    "\n",
    "Aplicações estratégicas:\n",
    "*   Segmentação de campanhas de marketing como frete grátis\n",
    "*   Ajuste de políticas comerciais e de frete\n",
    "*   Análise de potencial de mercado por região\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GWMBlYWBBgxx",
    "outputId": "3f4b655c-ed7b-49f4-d69b-e13286c5a748"
   },
   "outputs": [],
   "source": [
    "print(\"--- Estados Brasileiros com Maior Valor Médio de Pedido ---\")\n",
    "\n",
    "# Copiar DataFrames para evitar modificar os originais\n",
    "df_orders_eda = orders.copy()\n",
    "df_items_eda = order_items.copy()\n",
    "df_customers_eda = customers.copy()\n",
    "\n",
    "# Calcular o valor total por pedido (soma dos preços dos itens)\n",
    "valor_por_pedido = df_items_eda.groupby('order_id')['price'].sum().reset_index()\n",
    "valor_por_pedido.rename(columns={'price': 'valor_total_pedido'}, inplace=True)\n",
    "\n",
    "# Merge para adicionar customer_id ao valor do pedido\n",
    "pedidos_com_valor = pd.merge(\n",
    "    df_orders_eda[['order_id', 'customer_id']],\n",
    "    valor_por_pedido,\n",
    "    on='order_id',\n",
    "    how='inner' # Considera apenas pedidos que têm itens\n",
    ")\n",
    "\n",
    "# Merge para adicionar o estado do cliente\n",
    "pedidos_com_estado = pd.merge(\n",
    "    pedidos_com_valor,\n",
    "    df_customers_eda[['customer_id', 'customer_state']],\n",
    "    on='customer_id',\n",
    "    how='inner' # Considera apenas clientes que fizeram pedidos com itens\n",
    ")\n",
    "\n",
    "# Remover linhas com estado nulo, se houver\n",
    "pedidos_com_estado.dropna(subset=['customer_state'], inplace=True)\n",
    "\n",
    "# Calcula o valor médio do pedido por estado\n",
    "valor_medio_por_estado = pedidos_com_estado.groupby('customer_state')['valor_total_pedido'].mean().reset_index()\n",
    "\n",
    "# Ordena e pega os top 10 estados\n",
    "top_10_estados_valor_medio = valor_medio_por_estado.sort_values(by='valor_total_pedido', ascending=False).head(10)\n",
    "top_10_estados_valor_medio.rename(columns={'customer_state': 'estado', 'valor_total_pedido': 'valor_medio_pedido_R$'}, inplace=True)\n",
    "\n",
    "# Formata a coluna de valor médio como moeda brasileira para exibição\n",
    "top_10_estados_valor_medio_display = top_10_estados_valor_medio.copy()\n",
    "top_10_estados_valor_medio_display['valor_medio_pedido_R$'] = top_10_estados_valor_medio_display['valor_medio_pedido_R$'].map('R$ {:,.2f}'.format)\n",
    "\n",
    "print(\"\\nTop 10 Estados por Valor Médio de Pedido:\")\n",
    "print(top_10_estados_valor_medio_display)\n",
    "\n",
    "# Visualização\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x='valor_medio_pedido_R$',\n",
    "    y='estado',\n",
    "    data=top_10_estados_valor_medio, # Usar o DataFrame original para plotagem\n",
    "    hue='estado',\n",
    "    palette='coolwarm',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Top 10 Estados Brasileiros por Valor Médio de Pedido', fontsize=16)\n",
    "plt.xlabel('Valor Médio do Pedido (R$)', fontsize=14)\n",
    "plt.ylabel('Estado', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnálise do Valor Médio de Pedido por Estado:\")\n",
    "print(\"O gráfico e a tabela mostram os estados onde os clientes tendem a gastar mais por pedido, em média.\")\n",
    "print(\"Isso pode indicar maior poder aquisitivo, preferência por produtos mais caros, ou maior quantidade de itens por pedido nesses estados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8ChQSCmBhGU"
   },
   "source": [
    "#**Solução de Problemas de Negócio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf5acofJHPCY"
   },
   "source": [
    "##Análise de Retenção\n",
    "Este código realiza uma análise completa sobre a retenção de clientes em um e-commerce, combinando dados de pedidos e clientes para identificar padrões de compra recorrente. Com foco em clientes únicos (customer_unique_id) para evitar distorções e ter uma visualização clara do comportamento de compra.\n",
    "\n",
    "# **Insights:**\n",
    "*   Análise comparativa entre clientes únicos e recorrentes. Dos 96096 clientes únicos, 2997 (3.12%) realizaram mais de uma compra.\n",
    "*   Uma taxa de retenção baixa pode indicar oportunidades para melhorar a fidelidade do cliente através de programas de lealdade, melhorias no serviço ou ofertas personalizadas.\n",
    "\n",
    "\n",
    "Aplicações estratégicas:\n",
    "*   Identificação de oportunidades para melhorar a fidelização\n",
    "*   Análise de efetividade de estratégias de retenção já existentes\n",
    "*   Segmentação de clientes para campanhas personalizadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XnHomlVEBhYI",
    "outputId": "e53c3683-7e60-4424-d8f8-ce1d87a76a73"
   },
   "outputs": [],
   "source": [
    "print(\"--- Análise de Retenção de Clientes ---\")\n",
    "\n",
    "# Copiar DataFrames para evitar modificar os originais\n",
    "df_orders_retention = orders.copy()\n",
    "df_customers_retention = customers.copy()\n",
    "\n",
    "# Merge orders com customers para obter customer_unique_id\n",
    "# Seleciona apenas colunas necessárias\n",
    "orders_subset = df_orders_retention[['order_id', 'customer_id', 'order_purchase_timestamp']]\n",
    "customers_subset = df_customers_retention[['customer_id', 'customer_unique_id']]\n",
    "merged_df_retention = pd.merge(orders_subset, customers_subset, on='customer_id', how='inner')\n",
    "\n",
    "# Conta o número de pedidos por customer_unique_id\n",
    "pedidos_por_cliente_unico = merged_df_retention.groupby('customer_unique_id')['order_id'].count().reset_index(name='numero_de_pedidos')\n",
    "\n",
    "# Identifica clientes recorrentes (mais de 1 pedido)\n",
    "clientes_recorrentes_df = pedidos_por_cliente_unico[pedidos_por_cliente_unico['numero_de_pedidos'] > 1]\n",
    "\n",
    "numero_total_clientes_unicos = pedidos_por_cliente_unico['customer_unique_id'].nunique()\n",
    "numero_clientes_recorrentes = clientes_recorrentes_df['customer_unique_id'].nunique()\n",
    "\n",
    "taxa_retencao = 0\n",
    "if numero_total_clientes_unicos > 0:\n",
    "    taxa_retencao = (numero_clientes_recorrentes / numero_total_clientes_unicos) * 100\n",
    "\n",
    "print(f\"\\nNúmero total de clientes únicos: {numero_total_clientes_unicos}\")\n",
    "print(f\"Número de clientes recorrentes (mais de 1 pedido): {numero_clientes_recorrentes}\")\n",
    "print(f\"Taxa de retenção de clientes: {taxa_retencao:.2f}%\")\n",
    "\n",
    "print(\"\\nInsights sobre Retenção de Clientes:\")\n",
    "if taxa_retencao > 0:\n",
    "    print(f\"- Dos {numero_total_clientes_unicos} clientes únicos, {numero_clientes_recorrentes} ({taxa_retencao:.2f}%) realizaram mais de uma compra.\")\n",
    "    print(\"- Uma taxa de retenção baixa pode indicar oportunidades para melhorar a fidelidade do cliente através de programas de lealdade, melhorias no serviço ou ofertas personalizadas.\")\n",
    "else:\n",
    "    print(\"- Não foi possível calcular a taxa de retenção ou não há clientes recorrentes no período analisado.\")\n",
    "\n",
    "# Análise da distribuição do número de pedidos por cliente\n",
    "print(\"\\nDistribuição do Número de Pedidos por Cliente:\")\n",
    "distribuicao_pedidos = pedidos_por_cliente_unico['numero_de_pedidos'].value_counts().sort_index()\n",
    "print(distribuicao_pedidos)\n",
    "\n",
    "# Visualização da distribuição do número de pedidos\n",
    "plt.figure(figsize=(10, 6))\n",
    "distribuicao_pedidos_plot = distribuicao_pedidos.head(10) # Limitar para melhor visualização se houver muitos valores\n",
    "sns.barplot(x=distribuicao_pedidos_plot.index, y=distribuicao_pedidos_plot.values, palette='magma', hue=distribuicao_pedidos_plot.index, legend=False)\n",
    "plt.title('Distribuição do Número de Pedidos por Cliente Único', fontsize=15)\n",
    "plt.xlabel('Número de Pedidos Realizados', fontsize=12)\n",
    "plt.ylabel('Número de Clientes', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "for index, value in enumerate(distribuicao_pedidos_plot.values):\n",
    "    plt.text(index, value + 50, str(value), ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnálise da Distribuição:\")\n",
    "print(\"A grande maioria dos clientes fez apenas uma compra. Estratégias para incentivar uma segunda compra poderiam ter um impacto significativo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzTq0sMSHemR"
   },
   "source": [
    "##Predição de Atraso na Entrega\n",
    "\n",
    "Este código implementa um modelo de machine learning para prever atrasos na entrega dos pedidos. O processo inclui:\n",
    "\n",
    "####1. Preparação dos Dados:\n",
    "\n",
    "*   Relacionamento dos dados (pedidos, itens, produtos, vendedores, clientes);\n",
    "*   Definição da variável alvo (atrasado) comparando datas de entrega real e estimada.\n",
    "\n",
    "####2. Insigths;\n",
    "\n",
    "####3. Performance medida pela acurácia no conjunto de teste;\n",
    "\n",
    "####4. Análise de trade-off entre falsos positivos e falsos negativos;\n",
    "\n",
    "####5. Identificação de quais features mais impactam as previsões;\n",
    "\n",
    "####6. Aplicações Práticas;\n",
    "\n",
    "####7. Alertas proativos para possíveis atrasos;\n",
    "\n",
    "####8. Otimização logística com base nos fatores de risco identificados\n",
    "####9. Melhoria na experiência do cliente através de comunicações preventivas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7wiX9RfBHe4J",
    "outputId": "e486ee1e-1a57-496d-c5e2-92642780e8fd"
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreparando dados para o modelo de predição de atraso...\")\n",
    "\n",
    "# Copiar DataFrames para evitar modificar os originais no escopo global desta célula\n",
    "orders_df = orders.copy()\n",
    "order_items_df = order_items.copy()\n",
    "products_df = products.copy() # Deve conter 'product_category_name_final'\n",
    "sellers_df = sellers.copy()\n",
    "customers_df = customers.copy()\n",
    "\n",
    "# Selecionar colunas relevantes de 'orders' e remover NaNs cruciais para a definição de 'atrasado' e para a feature 'tempo_estimado_entrega_dias'.\n",
    "orders_subset = orders_df[['order_id', 'customer_id', 'order_purchase_timestamp', 'order_delivered_customer_date', 'order_estimated_delivery_date']].copy()\n",
    "orders_subset.dropna(subset=['order_delivered_customer_date', 'order_estimated_delivery_date', 'order_purchase_timestamp'], inplace=True)\n",
    "print(f\"Shape de orders_subset após dropna inicial: {orders_subset.shape}\")\n",
    "\n",
    "# Merge orders_subset com order_items\n",
    "data_modelo = pd.merge(orders_subset,\n",
    "                       order_items_df[['order_id', 'product_id', 'seller_id', 'price', 'freight_value']],\n",
    "                       on='order_id')\n",
    "print(f\"Shape após merge com order_items: {data_modelo.shape}\")\n",
    "\n",
    "# Merge data_modelo com products\n",
    "# Assegura que 'product_category_name_final' exista em products.\n",
    "if 'product_category_name_final' not in products_df.columns:\n",
    "    print(\"Alerta: 'product_category_name_final' não encontrada em 'products_df'. Verifique o pré-processamento.\")\n",
    "    # Tentativa de criar a coluna se não existir, assumindo 'outros' como fallback.\n",
    "    if 'product_category_name_english' in products_df.columns and 'product_category_name' in products_df.columns:\n",
    "         products_df['product_category_name_final'] = products_df['product_category_name_english'].fillna(products_df['product_category_name'])\n",
    "    elif 'product_category_name' in products_df.columns:\n",
    "        products_df['product_category_name_final'] = products_df['product_category_name']\n",
    "    else:\n",
    "        products_df['product_category_name_final'] = 'outros'\n",
    "    products_df['product_category_name_final'] = products_df['product_category_name_final'].fillna('outros')\n",
    "\n",
    "\n",
    "data_modelo = pd.merge(data_modelo,\n",
    "                       products_df[['product_id', 'product_category_name_final', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']],\n",
    "                       on='product_id', how='left')\n",
    "print(f\"Shape após merge com products: {data_modelo.shape}\")\n",
    "\n",
    "# Merge data_modelo com sellers\n",
    "data_modelo = pd.merge(data_modelo,\n",
    "                       sellers_df[['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']],\n",
    "                       on='seller_id', how='left')\n",
    "print(f\"Shape após merge com sellers: {data_modelo.shape}\")\n",
    "\n",
    "# Merge data_modelo com customers\n",
    "data_modelo = pd.merge(data_modelo,\n",
    "                       customers_df[['customer_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']],\n",
    "                       on='customer_id', how='left')\n",
    "print(f\"Shape após merge com customers: {data_modelo.shape}\")\n",
    "\n",
    "print(\"\\nMerge dos DataFrames concluído.\")\n",
    "\n",
    "# --- Definir Variável Alvo (`atrasado`) ---\n",
    "data_modelo['atrasado'] = (data_modelo['order_delivered_customer_date'] > data_modelo['order_estimated_delivery_date']).astype(int)\n",
    "print(f\"\\nVariável alvo 'atrasado' definida. Distribuição:\")\n",
    "print(data_modelo['atrasado'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# --- Engenharia de Features ---\n",
    "print(\"\\nIniciando engenharia de features...\")\n",
    "\n",
    "# Tempo estimado de entrega em dias\n",
    "data_modelo['tempo_estimado_entrega_dias'] = (data_modelo['order_estimated_delivery_date'] - data_modelo['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Volume do produto (cm³)\n",
    "# Tratar NaNs em dimensões antes de calcular o volume, preenchendo com 1\n",
    "dim_cols = ['product_length_cm', 'product_height_cm', 'product_width_cm']\n",
    "for col in dim_cols:\n",
    "    if col in data_modelo.columns:\n",
    "        data_modelo[col] = data_modelo[col].fillna(1)\n",
    "    else:\n",
    "        print(f\"Alerta: Coluna de dimensão '{col}' não encontrada. O volume do produto pode não ser calculado corretamente.\")\n",
    "        data_modelo[col] = 1\n",
    "data_modelo['product_volume_cm3'] = data_modelo['product_length_cm'] * data_modelo['product_height_cm'] * data_modelo['product_width_cm']\n",
    "\n",
    "# Final de semana (Sábado=5, Domingo=6)\n",
    "data_modelo['purchase_dayofweek'] = data_modelo['order_purchase_timestamp'].dt.dayofweek\n",
    "data_modelo['is_weekend_purchase'] = data_modelo['purchase_dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Entrega no mesmo estado?\n",
    "# Tratar NaNs em seller_state e customer_state antes da comparação, preenchendo com 'desconhecido'\n",
    "data_modelo['seller_state'] = data_modelo['seller_state'].fillna('desconhecido')\n",
    "data_modelo['customer_state'] = data_modelo['customer_state'].fillna('desconhecido')\n",
    "data_modelo['same_state_delivery'] = (data_modelo['seller_state'] == data_modelo['customer_state']).astype(int)\n",
    "\n",
    "print(\"Engenharia de features concluída.\")\n",
    "\n",
    "# Seleciona features para o modelo e a variável alvo\n",
    "features = [\n",
    "    'price', 'freight_value', 'product_weight_g', 'product_volume_cm3',\n",
    "    'tempo_estimado_entrega_dias', 'is_weekend_purchase', 'same_state_delivery',\n",
    "    'product_category_name_final', 'seller_state', 'customer_state'\n",
    "]\n",
    "target = 'atrasado'\n",
    "\n",
    "# Criar DataFrame final para o modelo e tratar NaNs nas features selecionadas (seguindo a lógica do script original)\n",
    "data_modelo_final = data_modelo[features + [target]].copy()\n",
    "print(f\"\\nShape antes de tratar NaNs nas features selecionadas para o modelo: {data_modelo_final.shape}\")\n",
    "print(\"Valores nulos ANTES do dropna específico para features do modelo:\")\n",
    "print(data_modelo_final[features].isnull().sum())\n",
    "\n",
    "data_modelo_final.dropna(subset=features, inplace=True)\n",
    "\n",
    "print(f\"\\nShape após tratar NaNs (dropna) nas features selecionadas: {data_modelo_final.shape}\")\n",
    "print(\"Valores nulos DEPOIS do dropna específico para features do modelo:\")\n",
    "print(data_modelo_final[features].isnull().sum())\n",
    "\n",
    "\n",
    "if len(data_modelo_final) < 100:\n",
    "    print(\"ALERTA: Dados insuficientes para treinar o modelo após preparação e remoção de NaNs.\")\n",
    "elif data_modelo_final[target].nunique() < 2:\n",
    "    print(f\"ALERTA: A variável alvo '{target}' tem menos de duas classes únicas após o pré-processamento. Não é possível treinar o modelo.\")\n",
    "else:\n",
    "    print(f\"Distribuição da variável alvo '{target}' em data_modelo_final (pronto para split):\\n{data_modelo_final[target].value_counts(normalize=True)}\")\n",
    "\n",
    "# --- Dividir Dataset em Treino e Teste ---\n",
    "if not data_modelo_final.empty and data_modelo_final[target].nunique() >= 2:\n",
    "    X = data_modelo_final[features]\n",
    "    y = data_modelo_final[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    print(f\"\\nDivisão treino/teste concluída:\")\n",
    "    print(f\"Tamanho do dataset de treino (X_train): {X_train.shape}\")\n",
    "    print(f\"Tamanho do dataset de teste (X_test): {X_test.shape}\")\n",
    "    print(f\"Distribuição da variável alvo no treino:\\n{y_train.value_counts(normalize=True) * 100}\")\n",
    "    print(f\"Distribuição da variável alvo no teste:\\n{y_test.value_counts(normalize=True) * 100}\")\n",
    "\n",
    "    # --- Construir e Treinar Modelo ---\n",
    "    print(\"\\nConstruindo e treinando o modelo de Regressão Logística...\")\n",
    "    numeric_features_pipeline = ['price', 'freight_value', 'product_weight_g', 'product_volume_cm3', 'tempo_estimado_entrega_dias']\n",
    "    categorical_features_pipeline = ['product_category_name_final', 'seller_state', 'customer_state']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                               ('scaler', StandardScaler())]), numeric_features_pipeline),\n",
    "            ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                               ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_features_pipeline) # sparse_output=False para facilitar a visualização das features transformadas, se necessário\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=42, solver='liblinear', max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    print(\"Treinamento do modelo concluído.\")\n",
    "\n",
    "    # --- Avaliar Performance do Modelo ---\n",
    "    print(\"\\nAvaliando performance do modelo...\")\n",
    "    y_pred_train = model_pipeline.predict(X_train)\n",
    "    y_pred_test = model_pipeline.predict(X_test)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    print(f\"\\nAcurácia no Treino: {accuracy_train:.4f}\")\n",
    "    print(f\"Acurácia no Teste: {accuracy_test:.4f}\")\n",
    "\n",
    "    print(\"\\nRelatório de Classificação (Teste):\")\n",
    "    target_names_report = ['Não Atrasado (0)', 'Atrasado (1)']\n",
    "    print(classification_report(y_test, y_pred_test, target_names=target_names_report))\n",
    "\n",
    "    print(\"\\nMatriz de Confusão (Teste):\")\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Previsto Não Atrasado', 'Previsto Atrasado'],\n",
    "                yticklabels=['Real Não Atrasado', 'Real Atrasado'])\n",
    "    plt.xlabel('Previsto')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de Confusão (Teste)')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Interpretação dos Resultados ---\")\n",
    "    print(f\"O modelo de Regressão Logística alcançou uma acurácia de {accuracy_test:.2%} no conjunto de teste.\")\n",
    "    print(\"Analise o Relatório de Classificação para entender a performance em cada classe.\")\n",
    "    print(\"  - Precisão (Precision) para 'Atrasado (1)': De todos os pedidos que o modelo previu como atrasados, qual percentual realmente atrasou?\")\n",
    "    print(\"  - Recall (Sensibilidade) para 'Atrasado (1)': De todos os pedidos que realmente atrasaram, qual percentual o modelo conseguiu identificar corretamente?\")\n",
    "    print(\"\\nA Matriz de Confusão detalha os acertos e erros:\")\n",
    "    print(f\"  - Verdadeiros Negativos (VN): {cm[0,0]} (Não atrasado, previsto como não atrasado)\")\n",
    "    print(f\"  - Falsos Positivos (FP):      {cm[0,1]} (Não atrasado, previsto como atrasado)\")\n",
    "    print(f\"  - Falsos Negativos (FN):      {cm[1,0]} (Atrasado, previsto como não atrasado)\")\n",
    "    print(f\"  - Verdadeiros Positivos (VP): {cm[1,1]} (Atrasado, previsto como atrasado)\")\n",
    "    print(\"\\nConsiderações:\")\n",
    "    print(\"- Se o recall para a classe 'Atrasado (1)' for baixo, o modelo pode estar falhando em identificar muitos dos pedidos que de fato atrasam.\")\n",
    "    print(\"- Melhorias podem incluir: engenharia de features mais avançada, outros algoritmos, ajuste de hiperparâmetros, tratamento de desbalanceamento de classes.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nALERTA FINAL: Modelo não treinado ou dados insuficientes/inválidos. Verifique as mensagens de alerta anteriores.\")\n",
    "\n",
    "print(\"\\n--- Fim da Seção 6.2 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7OWzZrLHfPx"
   },
   "source": [
    "##Segmentação de Clientes\n",
    "Este código realiza uma análise RFM (Recência, Frequência e Valor Monetário) para segmentar clientes de um e-commerce em grupos com comportamentos similares. Primeiro, ele verifica se os DataFrames necessários (orders, order_items, order_payments) estão disponíveis. Em seguida, calcula os componentes RFM:\n",
    "\n",
    "Recência: Dias desde a última compra (quanto menor, melhor).\n",
    "\n",
    "Frequência: Número total de pedidos.\n",
    "\n",
    "Valor Monetário: Total gasto pelo cliente.\n",
    "\n",
    "Os dados são pré-processados (tratamento de valores nulos e escalonamento) e, usando o método K-Means, os clientes são agrupados em clusters (o número ideal é definido pelo \"Método do Cotovelo\"). Finalmente, o código gera visualizações dos clusters (gráficos de barras) e um resumo estatístico, permitindo identificar perfis como \"Clientes Campeões\" (compras recentes, frequentes e de alto valor) ou \"Clientes em Risco\" (inatividade recente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-ca_sHHhHffN",
    "outputId": "becb900d-3955-4a7b-9248-abe8124693b7"
   },
   "outputs": [],
   "source": [
    "if 'orders' in globals() and 'order_items' in globals() and 'order_payments' in globals():\n",
    "    orders_df = orders.copy()\n",
    "    order_items_df = order_items.copy()\n",
    "    order_payments_df = order_payments.copy()\n",
    "    print(\"DataFrames 'orders', 'order_items' e 'order_payments' carregados e copiados para o escopo local da segmentação.\")\n",
    "else:\n",
    "    print(\"ERRO: DataFrames 'orders', 'order_items' ou 'order_payments' não encontrados no escopo global. Carregue-os primeiro.\")\n",
    "\n",
    "# Cálculo dos Componentes RFM\n",
    "print(\"Iniciando o cálculo dos componentes RFM...\")\n",
    "\n",
    "# Calcular Recência, Frequência e Valor Monetário (RFM)\n",
    "\n",
    "# Data da última compra no dataset + 1 dia para ser o ponto de referência 'agora'\n",
    "now = orders_df['order_purchase_timestamp'].max() + pd.Timedelta(days=1)\n",
    "print(f\"Data de referência para cálculo da Recência ('agora'): {now}\")\n",
    "\n",
    "# Agrupa por customer_id para calcular Recência e Frequência\n",
    "rfm_df = orders_df.groupby('customer_id').agg(\n",
    "    recencia=('order_purchase_timestamp', lambda x: (now - x.max()).days),\n",
    "    frequencia=('order_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Calcula Valor Monetário\n",
    "payments_agg = order_payments_df.groupby('order_id')['payment_value'].sum().reset_index()\n",
    "order_payments_merged = pd.merge(orders_df[['order_id', 'customer_id']], payments_agg, on='order_id', how='left')\n",
    "monetary_df = order_payments_merged.groupby('customer_id')['payment_value'].sum().reset_index()\n",
    "monetary_df.rename(columns={'payment_value': 'valor_monetario'}, inplace=True)\n",
    "\n",
    "# Junta os componentes R, F com M\n",
    "rfm_df = pd.merge(rfm_df, monetary_df, on='customer_id', how='left')\n",
    "rfm_df['valor_monetario'] = rfm_df['valor_monetario'].fillna(0)\n",
    "\n",
    "print(\"\\nDataFrame RFM criado:\")\n",
    "print(rfm_df.head())\n",
    "print(f\"\\nDimensões do DataFrame RFM: {rfm_df.shape}\")\n",
    "print(\"\\nEstatísticas descritivas do DataFrame RFM:\")\n",
    "print(rfm_df[['recencia', 'frequencia', 'valor_monetario']].describe())\n",
    "\n",
    "# Análise Exploratória Inicial do RFM\n",
    "### Análise Exploratória Inicial dos Dados RFM\n",
    "\n",
    "# Visualização das Distribuições RFM\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Distribuição dos Componentes RFM', fontsize=16)\n",
    "\n",
    "sns.histplot(rfm_df['recencia'], kde=True, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Distribuição da Recência (dias)')\n",
    "axes[0].set_xlabel('Recência (dias desde a última compra)')\n",
    "axes[0].set_ylabel('Número de Clientes')\n",
    "\n",
    "sns.histplot(rfm_df['frequencia'], kde=False, ax=axes[1], color='salmon', discrete=True)\n",
    "axes[1].set_title('Distribuição da Frequência (nº de pedidos)')\n",
    "axes[1].set_xlabel('Frequência (número de pedidos)')\n",
    "axes[1].set_ylabel('Número de Clientes')\n",
    "if rfm_df['frequencia'].max() < 20:\n",
    "    axes[1].set_xticks(range(1, rfm_df['frequencia'].max() + 1))\n",
    "\n",
    "sns.histplot(rfm_df['valor_monetario'], kde=True, ax=axes[2], color='lightgreen')\n",
    "axes[2].set_title('Distribuição do Valor Monetário (R$)')\n",
    "axes[2].set_xlabel('Valor Monetário Total Gasto (R$)')\n",
    "axes[2].set_ylabel('Número de Clientes')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComentários sobre as distribuições:\")\n",
    "print(\"- Recência: Geralmente mostra uma cauda longa.\")\n",
    "print(\"- Frequência: Frequentemente, a maioria dos clientes tem apenas uma compra.\")\n",
    "print(\"- Valor Monetário: Também pode ser assimétrico.\")\n",
    "\n",
    "# Pré-processamento dos Dados RFM (Imputação e Escalonamento)\n",
    "print(\"\\nIniciando o pré-processamento dos dados RFM...\")\n",
    "rfm_features = rfm_df[['recencia', 'frequencia', 'valor_monetario']].copy()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "rfm_imputed = imputer.fit_transform(rfm_features)\n",
    "rfm_imputed_df = pd.DataFrame(rfm_imputed, columns=rfm_features.columns)\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_imputed_df)\n",
    "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=rfm_features.columns)\n",
    "print(\"\\nDataFrame RFM escalado:\")\n",
    "print(rfm_scaled_df.head())\n",
    "print(\"\\nEstatísticas descritivas do DataFrame RFM escalado:\")\n",
    "print(rfm_scaled_df.describe())\n",
    "\n",
    "# Determinando o Número Ótimo de Clusters (Método do Cotovelo - Elbow Method)\n",
    "print(\"\\nDeterminando o número ótimo de clusters usando o Método do Cotovelo...\")\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(rfm_scaled_df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Número de Clusters (k)')\n",
    "plt.ylabel('Inércia')\n",
    "plt.title('Método do Cotovelo para Determinar k Ótimo')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"\\nInterpretação do Método do Cotovelo: Procure por um 'cotovelo' no gráfico.\")\n",
    "\n",
    "# Escolha do Número de Clusters\n",
    "### Escolha do Número de Clusters (k)\n",
    "\n",
    "# Aplicando K-Means Clustering\n",
    "optimal_k = 4\n",
    "print(f\"\\nNúmero ótimo de clusters escolhido: k = {optimal_k}\")\n",
    "print(\"Aplicando K-Means...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "rfm_df['cluster'] = kmeans.fit_predict(rfm_scaled_df)\n",
    "print(\"\\nSegmentação de clientes concluída.\")\n",
    "print(rfm_df.head())\n",
    "print(\"\\nContagem de clientes por cluster:\")\n",
    "print(rfm_df['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Análise dos Segmentos de Clientes\n",
    "print(\"\\nAnalisando os segmentos de clientes...\")\n",
    "rfm_clusters_summary = rfm_df.groupby('cluster').agg(\n",
    "    recencia_media=('recencia', 'mean'),\n",
    "    frequencia_media=('frequencia', 'mean'),\n",
    "    valor_monetario_medio=('valor_monetario', 'mean'),\n",
    "    contagem_clientes=('customer_id', 'count')\n",
    ").reset_index()\n",
    "print(\"\\nResumo dos clusters (médias RFM e contagem):\")\n",
    "print(rfm_clusters_summary)\n",
    "\n",
    "# Visualização das características dos clusters\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(f'Características Médias dos {optimal_k} Clusters RFM', fontsize=16)\n",
    "\n",
    "# Recência Média por Cluster\n",
    "sns.barplot(x='cluster', y='recencia_media', data=rfm_clusters_summary, ax=axes[0], hue='cluster', palette='coolwarm', legend=False) # Modificado\n",
    "axes[0].set_title('Recência Média por Cluster')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Recência Média (dias)')\n",
    "\n",
    "# Frequência Média por Cluster\n",
    "sns.barplot(x='cluster', y='frequencia_media', data=rfm_clusters_summary, ax=axes[1], hue='cluster', palette='coolwarm', legend=False) # Modificado\n",
    "axes[1].set_title('Frequência Média por Cluster')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Frequência Média (pedidos)')\n",
    "\n",
    "# Valor Monetário Médio por Cluster\n",
    "sns.barplot(x='cluster', y='valor_monetario_medio', data=rfm_clusters_summary, ax=axes[2], hue='cluster', palette='coolwarm', legend=False) # Modificado\n",
    "axes[2].set_title('Valor Monetário Médio por Cluster')\n",
    "axes[2].set_xlabel('Cluster')\n",
    "axes[2].set_ylabel('Valor Monetário Médio (R$)')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Interpretação dos Segmentos\n",
    "### Interpretação dos Segmentos de Clientes\n",
    "\n",
    "#**Exemplo de Interpretação:**\n",
    "#*   **Cluster 0 - Clientes Campeões/Fiéis:** Baixa Recência, Alta Frequência, Alto Valor Monetário.\n",
    "#*   **Cluster 1 - Clientes em Risco/Precisam de Atenção:** Recência Média/Alta, Frequência Média/Baixa.\n",
    "#*   **Cluster 2 - Novos Clientes/Potencial:** Recência Baixa/Média, Baixa Frequência.\n",
    "#*   **Cluster 3 - Clientes Perdidos/Hibernando:** Alta Recência, Baixa Frequência, Baixo Valor Monetário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qedYJ_HCHfrV"
   },
   "source": [
    "##Análise de Satisfação\n",
    "Este código analisa a satisfação de clientes de um e-commerce, cruzando dados de avaliações (review_score) com métricas de entrega, categorias de produtos e valores financeiros. Primeiro, ele prepara os dados, calculando o tempo real de entrega e a diferença em relação à estimativa. Em seguida, gera visualizações que mostram:\n",
    "\n",
    "Distribuição das notas: A maioria dos clientes dá nota 5, mas há um número significativo de notas 1.\n",
    "\n",
    "Relação entre notas e entrega: Notas baixas estão associadas a prazos mais longos e atrasos.\n",
    "\n",
    "Desempenho por categoria: Identifica as categorias de produtos com melhores e piores avaliações médias.\n",
    "\n",
    "Impacto de preço e frete: Verifica se valores muito altos ou baixos influenciam as notas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bPUq39-7Hf5X",
    "outputId": "8b6d6ab0-a459-49e1-e3c0-540237f0cbb7"
   },
   "outputs": [],
   "source": [
    "df_satisfacao = pd.merge(order_reviews, orders, on='order_id', how='left')\n",
    "\n",
    "# Converter colunas de data para datetime\n",
    "date_cols_satisfacao = ['order_purchase_timestamp', 'order_delivered_customer_date',\n",
    "                        'order_estimated_delivery_date', 'review_creation_date',\n",
    "                        'review_answer_timestamp']\n",
    "for col in date_cols_satisfacao:\n",
    "    if col in df_satisfacao.columns:\n",
    "        df_satisfacao[col] = pd.to_datetime(df_satisfacao[col], errors='coerce')\n",
    "\n",
    "# Calcular tempo de entrega real e diferença da estimativa\n",
    "df_satisfacao['tempo_entrega_real_dias'] = (df_satisfacao['order_delivered_customer_date'] - df_satisfacao['order_purchase_timestamp']).dt.days\n",
    "df_satisfacao['diferenca_estimativa_entrega_dias'] = (df_satisfacao['order_delivered_customer_date'] - df_satisfacao['order_estimated_delivery_date']).dt.days\n",
    "\n",
    "print(f\"Dados de satisfação preparados. Total de reviews: {len(df_satisfacao)}\")\n",
    "print(df_satisfacao[['order_id', 'review_score', 'tempo_entrega_real_dias', 'diferenca_estimativa_entrega_dias']].head())\n",
    "\n",
    "# Análise da Pontuação de Avaliação (Review Score) ---\n",
    "print(\"\\nAnalisando a distribuição da Pontuação de Avaliação (Review Score)...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='review_score', data=df_satisfacao, palette='viridis', hue='review_score', legend=False)\n",
    "plt.title('Distribuição da Pontuação de Avaliação (Review Score)', fontsize=15)\n",
    "plt.xlabel('Review Score', fontsize=12)\n",
    "plt.ylabel('Contagem', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretação da Distribuição do Review Score:\")\n",
    "print(\"- A maioria dos clientes atribui a nota máxima (5 estrelas).\")\n",
    "print(\"- Há uma quantidade significativa de notas 1 estrela, indicando insatisfação em alguns casos.\")\n",
    "print(\"- Notas intermediárias (2, 3, 4) são menos comuns.\")\n",
    "\n",
    "# Relação entre Review Score e Métricas de Entrega ---\n",
    "print(\"\\nAnalisando Review Score vs. Métricas de Entrega...\")\n",
    "df_entrega_plot = df_satisfacao.dropna(subset=['review_score', 'tempo_entrega_real_dias', 'diferenca_estimativa_entrega_dias'])\n",
    "\n",
    "if not df_entrega_plot.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    fig.suptitle('Review Score vs. Métricas de Entrega', fontsize=16)\n",
    "\n",
    "    # Tempo de Entrega Real vs. Review Score\n",
    "    sns.boxplot(x='review_score', y='tempo_entrega_real_dias', data=df_entrega_plot, ax=axes[0], hue='review_score', palette='coolwarm', legend=False)\n",
    "    axes[0].set_title('Tempo de Entrega Real (dias) por Review Score', fontsize=14)\n",
    "    axes[0].set_xlabel('Review Score', fontsize=12)\n",
    "    axes[0].set_ylabel('Tempo de Entrega Real (dias)', fontsize=12)\n",
    "    if not df_entrega_plot.empty:\n",
    "        axes[0].set_ylim(0, df_entrega_plot['tempo_entrega_real_dias'].quantile(0.95))\n",
    "\n",
    "    # Diferença da Estimativa de Entrega vs. Review Score\n",
    "    sns.boxplot(x='review_score', y='diferenca_estimativa_entrega_dias', data=df_entrega_plot, ax=axes[1], hue='review_score', palette='coolwarm', legend=False)\n",
    "    axes[1].set_title('Diferença da Estimativa de Entrega (dias) por Review Score', fontsize=14)\n",
    "    axes[1].set_xlabel('Review Score', fontsize=12)\n",
    "    axes[1].set_ylabel('Diferença da Estimativa (dias) [Real - Estimada]', fontsize=12)\n",
    "    if not df_entrega_plot.empty:\n",
    "        lower_q = df_entrega_plot['diferenca_estimativa_entrega_dias'].quantile(0.05)\n",
    "        upper_q = df_entrega_plot['diferenca_estimativa_entrega_dias'].quantile(0.95)\n",
    "        axes[1].set_ylim(lower_q, upper_q)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nInterpretação - Review Score vs. Métricas de Entrega:\")\n",
    "    print(\"- Tempo de Entrega: Notas mais baixas (1, 2) tendem a estar associadas a tempos de entrega mais longos.\")\n",
    "    print(\"- Diferença da Estimativa: Atrasos (valores positivos) estão fortemente correlacionados com notas baixas.\")\n",
    "else:\n",
    "    print(\"Dados insuficientes para plotar Review Score vs. Métricas de Entrega.\")\n",
    "\n",
    "# --- Relação entre Review Score e Categoria do Produto ---\n",
    "print(\"\\nPreparando dados para análise de Review Score por Categoria de Produto...\")\n",
    "df_satisfacao_items = pd.merge(df_satisfacao, order_items[['order_id', 'product_id']], on='order_id', how='left')\n",
    "df_satisfacao_items = pd.merge(df_satisfacao_items, products[['product_id', 'product_category_name']], on='product_id', how='left')\n",
    "df_satisfacao_items = pd.merge(df_satisfacao_items, product_category_name_translation[['product_category_name', 'product_category_name_english']], on='product_category_name', how='left')\n",
    "df_satisfacao_items.rename(columns={'product_category_name_english': 'categoria_produto_ingles'}, inplace=True)\n",
    "df_satisfacao_items['categoria_produto_ingles'] = df_satisfacao_items['categoria_produto_ingles'].fillna('outros')\n",
    "\n",
    "df_categoria_plot = df_satisfacao_items.dropna(subset=['review_score', 'categoria_produto_ingles'])\n",
    "\n",
    "if not df_categoria_plot.empty and df_categoria_plot['categoria_produto_ingles'].nunique() > 1:\n",
    "    media_score_categoria = df_categoria_plot.groupby('categoria_produto_ingles')['review_score'].agg(['mean', 'count']).sort_values(by='mean')\n",
    "\n",
    "    # Filtrar categorias com poucas avaliações para uma análise mais estável\n",
    "    media_score_categoria_filtrada = media_score_categoria[media_score_categoria['count'] >= 20] # Limite de contagem ajustável\n",
    "\n",
    "    if not media_score_categoria_filtrada.empty:\n",
    "        top_n_cat = min(7, len(media_score_categoria_filtrada))\n",
    "        bottom_n_cat = min(7, len(media_score_categoria_filtrada))\n",
    "\n",
    "        top_categorias = media_score_categoria_filtrada.tail(top_n_cat)\n",
    "        bottom_categorias = media_score_categoria_filtrada.head(bottom_n_cat)\n",
    "\n",
    "        print(f\"\\nTop {top_n_cat} Categorias por Média de Review Score (com >= 20 avaliações):\")\n",
    "        print(top_categorias)\n",
    "        print(f\"\\nBottom {bottom_n_cat} Categorias por Média de Review Score (com >= 20 avaliações):\")\n",
    "        print(bottom_categorias)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 16))\n",
    "        fig.suptitle('Review Score Médio por Categoria de Produto (Top e Bottom com >= 20 avaliações)', fontsize=16)\n",
    "\n",
    "        sns.boxplot(x='review_score', y='categoria_produto_ingles',\n",
    "                    data=df_categoria_plot[df_categoria_plot['categoria_produto_ingles'].isin(top_categorias.index)],\n",
    "                    ax=axes[0], palette='summer', order=top_categorias.index.tolist()[::-1], hue='review_score', legend=False)\n",
    "        axes[0].set_title(f'Top {top_n_cat} Categorias com Melhores Avaliações Médias', fontsize=14)\n",
    "        axes[0].set_xlabel('Review Score', fontsize=12)\n",
    "        axes[0].set_ylabel('Categoria do Produto (Inglês)', fontsize=12)\n",
    "\n",
    "        sns.boxplot(x='review_score', y='categoria_produto_ingles',\n",
    "                    data=df_categoria_plot[df_categoria_plot['categoria_produto_ingles'].isin(bottom_categorias.index)],\n",
    "                    ax=axes[1], palette='autumn_r', order=bottom_categorias.index.tolist(), hue='review_score', legend=False)\n",
    "        axes[1].set_title(f'Bottom {bottom_n_cat} Categorias com Piores Avaliações Médias', fontsize=14)\n",
    "        axes[1].set_xlabel('Review Score', fontsize=12)\n",
    "        axes[1].set_ylabel('Categoria do Produto (Inglês)', fontsize=12)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "        print(\"\\nInterpretação - Review Score vs. Categoria: Certas categorias podem ter melhor ou pior avaliação média.\")\n",
    "    else:\n",
    "        print(\"Poucas categorias com volume suficiente de avaliações para análise de Top/Bottom.\")\n",
    "else:\n",
    "    print(\"Dados insuficientes para a análise de Review Score por Categoria de Produto.\")\n",
    "\n",
    "# --- Relação entre Review Score e Valores (Preço e Frete) ---\n",
    "print(\"\\nPreparando dados para análise de Review Score vs. Valores Financeiros...\")\n",
    "df_order_payment_totals = order_payments.groupby('order_id')['payment_value'].sum().reset_index()\n",
    "df_order_payment_totals.rename(columns={'payment_value': 'total_price_order'}, inplace=True)\n",
    "\n",
    "df_order_freight_totals = order_items.groupby('order_id')['freight_value'].sum().reset_index()\n",
    "df_order_freight_totals.rename(columns={'freight_value': 'total_freight_order'}, inplace=True)\n",
    "\n",
    "df_satisfacao_finance = pd.merge(df_satisfacao, df_order_payment_totals, on='order_id', how='left')\n",
    "df_satisfacao_finance = pd.merge(df_satisfacao_finance, df_order_freight_totals, on='order_id', how='left')\n",
    "\n",
    "df_finance_plot = df_satisfacao_finance.dropna(subset=['review_score', 'total_price_order', 'total_freight_order'])\n",
    "df_finance_plot_filtered = pd.DataFrame() # Inicializa\n",
    "\n",
    "if not df_finance_plot.empty:\n",
    "    price_cap = df_finance_plot['total_price_order'].quantile(0.95)\n",
    "    freight_cap = df_finance_plot['total_freight_order'].quantile(0.95)\n",
    "    df_finance_plot_filtered = df_finance_plot[\n",
    "        (df_finance_plot['total_price_order'] <= price_cap) & (df_finance_plot['total_price_order'] > 0) &\n",
    "        (df_finance_plot['total_freight_order'] <= freight_cap) & (df_finance_plot['total_freight_order'] >= 0)\n",
    "    ]\n",
    "\n",
    "print(\"\\nAnalisando Review Score vs. Preço Total e Frete Total do Pedido...\")\n",
    "if not df_finance_plot_filtered.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    fig.suptitle('Review Score vs. Valores Financeiros do Pedido', fontsize=16)\n",
    "\n",
    "    sns.boxplot(x='review_score', y='total_price_order', data=df_finance_plot_filtered, ax=axes[0], hue='review_score', palette='plasma_r', legend=False, showfliers=False)\n",
    "    axes[0].set_title('Preço Total do Pedido por Review Score', fontsize=14)\n",
    "    axes[0].set_xlabel('Review Score', fontsize=12)\n",
    "    axes[0].set_ylabel('Preço Total do Pedido (R$)', fontsize=12)\n",
    "\n",
    "    sns.boxplot(x='review_score', y='total_freight_order', data=df_finance_plot_filtered, ax=axes[1], hue='review_score', palette='plasma_r', legend=False, showfliers=False)\n",
    "    axes[1].set_title('Frete Total do Pedido por Review Score', fontsize=14)\n",
    "    axes[1].set_xlabel('Review Score', fontsize=12)\n",
    "    axes[1].set_ylabel('Frete Total do Pedido (R$)', fontsize=12)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    print(\"\\nInterpretação - Review Score vs. Valores: Analisar se preços ou fretes muito altos/baixos se correlacionam com notas específicas.\")\n",
    "else:\n",
    "    print(\"Dados insuficientes para a análise de Review Score por Valores Financeiros após filtragem.\")\n",
    "\n",
    "print(\"\\nAnálise de Satisfação do Cliente Concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXpqKWo6HgDv"
   },
   "source": [
    "#**Visualização e Dashboards**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He65TsDwynl1"
   },
   "source": [
    "# Dashboard geral de Evolução das Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c22a5d018cbb4cafaf214f05f12dda0f",
      "dce5b305d98d4a38839e4fb09b29621b",
      "f60ea16829c04cf096ebfbce684c9d9d",
      "3c884157d28e4c6e8b37704b93d1268b",
      "fb1ef46dbb914a8a8364daaa092cd2ee",
      "2622b8414547467fb64bdec6375dc641",
      "814e074d43a945bb82809a1084e4eab8",
      "02b72765064c474fa1042fc59cb65513",
      "f513bdebbd674c3098102e6d10068a1c",
      "8587ab1299c447239eab8637cf81c191",
      "dd2960f297a2474a9120d7afd79328c1",
      "d69989bb3b4947a7a106cc317dd36b15",
      "95f7b878c5814a1ea1d5438269730837",
      "ddd0ddeaadf8419f84d3df8ad72db21c",
      "bf020e8781de4a2898dd96742eddb248",
      "992e50944dbf49aab1757cbdd38d8135",
      "0499ea00c28e40b4bfae08612d23f3bb",
      "aa72eaf47b0045a39c9b67e044e2a6a7",
      "e4e4eedbb295428d88cbcf01f33ca08e",
      "2c9c73862a98410a800db17ea64e9b04",
      "daa20f780e3d4a61a92ef9f065db1ad8",
      "6f918208f08f412bb990b96be2e8baf1",
      "95ce9be1a8c845649a978dc0db38ff44",
      "bcf3657fecf742c9a49a82e08f47bd9c",
      "38d5392e3cb640b5acfaf87cb7d183b1",
      "99f46db81a044a708169a9f74b1776d1",
      "66425d9ff6bd46c4a3ec499ffb2f18b2",
      "1f8c1e02c3b445459d353955fadb3639"
     ]
    },
    "id": "JOO5fu2D5H3o",
    "outputId": "a79541c6-bd8e-4546-c187-fdca71492de7"
   },
   "outputs": [],
   "source": [
    "# Configuração do estilo Seaborn\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Função para formatar valores monetários\n",
    "def format_currency(x, pos):\n",
    "    return f'R$ {x:,.0f}'\n",
    "\n",
    "# Preparar dados para o dashboard\n",
    "print(\"Preparando dados para o dashboard de vendas...\")\n",
    "\n",
    "# Verificar se os DataFrames necessários estão disponíveis\n",
    "required_dfs = ['orders', 'order_items', 'products', 'customers', 'product_category_name_translation']\n",
    "\n",
    "all_dfs_loaded = True\n",
    "for df_name in required_dfs:\n",
    "    if df_name not in globals():\n",
    "        print(f\"Erro: DataFrame '{df_name}' não encontrado. Certifique-se de carregar todos os dados necessários.\")\n",
    "        all_dfs_loaded = False\n",
    "\n",
    "if not all_dfs_loaded:\n",
    "    print(\"Não é possível continuar com a criação do dashboard devido a DataFrames ausentes.\")\n",
    "else:\n",
    "    # Cria DataFrame base para o dashboard\n",
    "    dashboard_data = pd.merge(orders[['order_id', 'customer_id', 'order_purchase_timestamp']],\n",
    "                              order_items[['order_id', 'product_id', 'price']],\n",
    "                              on='order_id')\n",
    "    dashboard_data = pd.merge(dashboard_data,\n",
    "                              products[['product_id', 'product_category_name']],\n",
    "                              on='product_id')\n",
    "    dashboard_data = pd.merge(dashboard_data,\n",
    "                              customers[['customer_id', 'customer_state']],\n",
    "                              on='customer_id')\n",
    "    dashboard_data = pd.merge(dashboard_data,\n",
    "                              product_category_name_translation[['product_category_name', 'product_category_name_english']],\n",
    "                              on='product_category_name',\n",
    "                              how='left')\n",
    "\n",
    "    # Tratamento de dados\n",
    "    dashboard_data['product_category_name_english'] = dashboard_data['product_category_name_english'].fillna('undefined')\n",
    "    dashboard_data['order_purchase_timestamp'] = pd.to_datetime(dashboard_data['order_purchase_timestamp'])\n",
    "    dashboard_data['month_year'] = dashboard_data['order_purchase_timestamp'].dt.to_period('M')\n",
    "    dashboard_data['month_year_formatted'] = dashboard_data['order_purchase_timestamp'].dt.strftime('%b/%Y')\n",
    "\n",
    "    ALL_OPTION_TEXT = \"Todos\"\n",
    "\n",
    "    states_list_unique = sorted(dashboard_data['customer_state'].unique())\n",
    "    states_options = [ALL_OPTION_TEXT] + states_list_unique\n",
    "\n",
    "    categories_list_unique = sorted(dashboard_data['product_category_name_english'].unique())\n",
    "    categories_options = [ALL_OPTION_TEXT] + categories_list_unique\n",
    "\n",
    "    # Criar widgets para filtros\n",
    "    state_selector = widgets.SelectMultiple(\n",
    "        options=states_options,\n",
    "        value=[ALL_OPTION_TEXT],\n",
    "        description='Estados:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='300px', height='150px')\n",
    "    )\n",
    "\n",
    "    category_selector = widgets.SelectMultiple(\n",
    "        options=categories_options,\n",
    "        value=[ALL_OPTION_TEXT],\n",
    "        description='Categorias:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='300px', height='150px')\n",
    "    )\n",
    "\n",
    "    apply_button = widgets.Button(\n",
    "        description='Aplicar Filtros',\n",
    "        button_style='success',\n",
    "        tooltip='Clique para aplicar os filtros selecionados'\n",
    "    )\n",
    "\n",
    "    # Container para os filtros\n",
    "    filter_container = widgets.VBox([\n",
    "        widgets.HTML(value=\"<h3>Filtros do Dashboard</h3>\"),\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([widgets.Label(\"Selecione os Estados:\"), state_selector]),\n",
    "            widgets.VBox([widgets.Label(\"Selecione as Categorias:\"), category_selector])\n",
    "        ]),\n",
    "        apply_button\n",
    "    ])\n",
    "\n",
    "\n",
    "    dashboard_output = widgets.Output()\n",
    "\n",
    "    fig_dashboard = None\n",
    "    gs_dashboard = None\n",
    "    kpi_area_dash, kpi_area2_dash, kpi_area3_dash = None, None, None\n",
    "    ax1_dash, ax2_dash = None, None\n",
    "    footer_ax_dash = None\n",
    "\n",
    "    def create_dashboard_layout_once():\n",
    "        global fig_dashboard, gs_dashboard, kpi_area_dash, kpi_area2_dash, kpi_area3_dash, ax1_dash, ax2_dash, footer_ax_dash\n",
    "        if fig_dashboard is None:\n",
    "            fig_dashboard = plt.figure(figsize=(18, 11), facecolor='white', constrained_layout=True)\n",
    "            gs_dashboard = gridspec.GridSpec(4, 6, figure=fig_dashboard)\n",
    "\n",
    "            kpi_area_dash = fig_dashboard.add_subplot(gs_dashboard[0, 0:2])\n",
    "            kpi_area2_dash = fig_dashboard.add_subplot(gs_dashboard[0, 2:4])\n",
    "            kpi_area3_dash = fig_dashboard.add_subplot(gs_dashboard[0, 4:6])\n",
    "            ax1_dash = fig_dashboard.add_subplot(gs_dashboard[1:3, 0:3])\n",
    "            ax2_dash = fig_dashboard.add_subplot(gs_dashboard[1:3, 3:6])\n",
    "            footer_ax_dash = fig_dashboard.add_subplot(gs_dashboard[3, :])\n",
    "\n",
    "    # Função para atualizar o dashboard com base nos filtros\n",
    "    def update_dashboard_on_filter(b):\n",
    "        global fig_dashboard, kpi_area_dash, kpi_area2_dash, kpi_area3_dash, ax1_dash, ax2_dash, footer_ax_dash\n",
    "\n",
    "        if fig_dashboard is None:\n",
    "            create_dashboard_layout_once()\n",
    "\n",
    "        selected_states_raw = list(state_selector.value)\n",
    "        selected_categories_raw = list(category_selector.value)\n",
    "\n",
    "        apply_state_filter = bool(selected_states_raw) and ALL_OPTION_TEXT not in selected_states_raw\n",
    "        actual_selected_states = selected_states_raw if apply_state_filter else []\n",
    "\n",
    "        apply_category_filter = bool(selected_categories_raw) and ALL_OPTION_TEXT not in selected_categories_raw\n",
    "        actual_selected_categories = selected_categories_raw if apply_category_filter else []\n",
    "\n",
    "        # Aplicar filtros aos dados\n",
    "        filtered_data = dashboard_data.copy()\n",
    "\n",
    "        if apply_state_filter:\n",
    "            filtered_data = filtered_data[filtered_data['customer_state'].isin(actual_selected_states)]\n",
    "\n",
    "        if apply_category_filter:\n",
    "            filtered_data = filtered_data[filtered_data['product_category_name_english'].isin(actual_selected_categories)]\n",
    "\n",
    "        # Recalcular agregações\n",
    "        monthly_filtered = filtered_data.groupby('month_year')['price'].sum().reset_index()\n",
    "        monthly_filtered['month_year_str'] = monthly_filtered['month_year'].astype(str)\n",
    "\n",
    "        category_filtered = filtered_data.groupby('product_category_name_english')['price'].sum().reset_index()\n",
    "        category_filtered = category_filtered[category_filtered['product_category_name_english'] != 'undefined']\n",
    "        category_filtered = category_filtered.sort_values('price', ascending=False).head(15).sort_values('price', ascending=True)\n",
    "\n",
    "\n",
    "        # Calcular KPIs\n",
    "        total_sales = filtered_data['price'].sum()\n",
    "        avg_ticket = filtered_data.groupby('order_id')['price'].sum().mean() if not filtered_data.empty and 'order_id' in filtered_data else 0\n",
    "        total_orders = filtered_data['order_id'].nunique()\n",
    "\n",
    "        with dashboard_output:\n",
    "            dashboard_output.clear_output(wait=True)\n",
    "            all_axes = [kpi_area_dash, kpi_area2_dash, kpi_area3_dash, ax1_dash, ax2_dash, footer_ax_dash]\n",
    "            for current_ax in all_axes:\n",
    "                current_ax.clear()\n",
    "                current_ax.grid(False)\n",
    "\n",
    "            fig_dashboard.suptitle('DASHBOARD DE EVOLUÇÃO DE VENDAS', fontsize=20, fontweight='bold')\n",
    "\n",
    "            kpi_area_dash.axis('off')\n",
    "            kpi_area2_dash.axis('off')\n",
    "            kpi_area3_dash.axis('off')\n",
    "            footer_ax_dash.axis('off')\n",
    "\n",
    "            kpi_area_dash.text(0.5, 0.5, f'Vendas Totais\\n{format_currency(total_sales, None)}',\n",
    "                         fontsize=14, fontweight='bold', ha='center', va='center',\n",
    "                         bbox=dict(facecolor='lightblue', alpha=0.5, boxstyle='round,pad=0.5'))\n",
    "\n",
    "            kpi_area2_dash.text(0.5, 0.5, f'Ticket Médio\\n{format_currency(avg_ticket, None)}',\n",
    "                          fontsize=14, fontweight='bold', ha='center', va='center',\n",
    "                          bbox=dict(facecolor='lightgreen', alpha=0.5, boxstyle='round,pad=0.5'))\n",
    "\n",
    "            kpi_area3_dash.text(0.5, 0.5, f'Total de Pedidos\\n{total_orders:,}',\n",
    "                          fontsize=14, fontweight='bold', ha='center', va='center',\n",
    "                          bbox=dict(facecolor='lightsalmon', alpha=0.5, boxstyle='round,pad=0.5'))\n",
    "\n",
    "            ax1_dash.set_title('Evolução Mensal de Vendas', fontsize=14, pad=10)\n",
    "            ax1_dash.set_xlabel('Período', fontsize=12)\n",
    "            ax1_dash.set_ylabel('Total de Vendas (R$)', fontsize=12)\n",
    "            ax1_dash.yaxis.set_major_formatter(mtick.FuncFormatter(format_currency))\n",
    "            if not monthly_filtered.empty:\n",
    "                sns.lineplot(x='month_year_str', y='price', data=monthly_filtered, marker='o', ax=ax1_dash, linewidth=2, color=sns.color_palette('viridis')[0])\n",
    "                ax1_dash.tick_params(axis='x', rotation=45)\n",
    "                plt.setp(ax1_dash.get_xticklabels(), ha='right')\n",
    "                for i, row in monthly_filtered.iterrows():\n",
    "                    ax1_dash.annotate(f'{format_currency(row[\"price\"], None)}',\n",
    "                                (row['month_year_str'], row['price']),\n",
    "                                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n",
    "            else:\n",
    "                ax1_dash.text(0.5, 0.5, 'Sem dados para o período selecionado', ha='center', va='center', fontsize=12)\n",
    "            ax1_dash.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "            ax2_dash.set_title('Top Categorias por Vendas', fontsize=14, pad=10)\n",
    "            ax2_dash.set_xlabel('Total de Vendas (R$)', fontsize=12)\n",
    "            ax2_dash.set_ylabel('Categoria de Produto', fontsize=12)\n",
    "            ax2_dash.xaxis.set_major_formatter(mtick.FuncFormatter(format_currency))\n",
    "            if not category_filtered.empty:\n",
    "                bars = sns.barplot(x='price', y='product_category_name_english', data=category_filtered, ax=ax2_dash,\n",
    "                                   palette='viridis', hue='product_category_name_english', dodge=False, legend=False)\n",
    "                for i, p in enumerate(bars.patches):\n",
    "                    width = p.get_width()\n",
    "                    ax2_dash.text(width + (total_sales * 0.005 if total_sales else 10), # Dynamic offset\n",
    "                                  p.get_y() + p.get_height()/2,\n",
    "                                  f'{format_currency(width, None)}',\n",
    "                                  ha='left', va='center', fontsize=8)\n",
    "            else:\n",
    "                ax2_dash.text(0.5, 0.5, 'Sem dados para as categorias selecionadas', ha='center', va='center', fontsize=12)\n",
    "            ax2_dash.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "            filter_text_display = \"Filtros aplicados: \"\n",
    "            if apply_state_filter:\n",
    "                filter_text_display += f\"Estados: {', '.join(actual_selected_states)} | \"\n",
    "            else:\n",
    "                filter_text_display += \"Todos os Estados | \"\n",
    "\n",
    "            if apply_category_filter:\n",
    "                filter_text_display += f\"Categorias: {', '.join(actual_selected_categories[:3])}\"\n",
    "                if len(actual_selected_categories) > 3:\n",
    "                    filter_text_display += f\" e mais {len(actual_selected_categories)-3}...\"\n",
    "            else:\n",
    "                filter_text_display += \"Todas as Categorias\"\n",
    "\n",
    "            min_date_str = dashboard_data['order_purchase_timestamp'].min().strftime('%d/%m/%Y') if not dashboard_data.empty else \"N/A\"\n",
    "            max_date_str = dashboard_data['order_purchase_timestamp'].max().strftime('%d/%m/%Y') if not dashboard_data.empty else \"N/A\"\n",
    "            date_range_text = f\"Período dos dados: {min_date_str} a {max_date_str}\"\n",
    "\n",
    "            footer_ax_dash.text(0.01, 0.7, filter_text_display, fontsize=10, ha='left', va='center', wrap=True)\n",
    "            footer_ax_dash.text(0.01, 0.4, date_range_text, fontsize=10, ha='left', va='center')\n",
    "            footer_ax_dash.text(0.99, 0.1, f\"Atualizado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\", fontsize=9, ha='right', va='center', color='gray')\n",
    "\n",
    "\n",
    "            display(fig_dashboard)\n",
    "\n",
    "\n",
    "    if all_dfs_loaded:\n",
    "        create_dashboard_layout_once()\n",
    "\n",
    "\n",
    "        apply_button.on_click(update_dashboard_on_filter)\n",
    "\n",
    "\n",
    "        display(filter_container)\n",
    "        display(dashboard_output)\n",
    "\n",
    "        update_dashboard_on_filter(None)\n",
    "\n",
    "        print(\"\\nDashboard de vendas interativo carregado. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJU6xZMnBKJx"
   },
   "source": [
    "Mapa de calor mostrando a concentração de vendas por região/estado do Brasil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 998
    },
    "id": "4NRmGSUvBKX5",
    "outputId": "0c6c1850-8f41-4b5d-e845-eeb11b8dd50c"
   },
   "outputs": [],
   "source": [
    "def gerar_mapa_densidade_clientes():\n",
    "    \"\"\"\n",
    "    Gera um mapa de densidade de kernel (heatmap de pontos) mostrando a concentração\n",
    "    geográfica de clientes no Brasil.\n",
    "    Utiliza dados reais se 'customers' e 'geolocation_agg' estiverem disponíveis e mescláveis.\n",
    "    Caso contrário, simula pontos aleatórios dentro dos limites do Brasil.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Gerando Mapa de Densidade de Clientes (Kernel Heatmap) ---\")\n",
    "\n",
    "    # Verificar se os DataFrames necessários estão carregados\n",
    "    required_dfs_mapa = ['customers', 'geolocation_agg']\n",
    "    dfs_disponiveis = True\n",
    "    for df_name in required_dfs_mapa:\n",
    "        if df_name not in globals():\n",
    "            print(f\"DataFrame '{df_name}' não encontrado globalmente. Necessário para o mapa de densidade com dados reais.\")\n",
    "            dfs_disponiveis = False\n",
    "            break\n",
    "\n",
    "    locations_data = None\n",
    "\n",
    "    if dfs_disponiveis:\n",
    "        print(\"Processando dados reais para o mapa de densidade...\")\n",
    "        customers_df = globals()['customers']\n",
    "        geolocation_agg_df = globals()['geolocation_agg']\n",
    "\n",
    "        if 'customer_zip_code_prefix' not in customers_df.columns:\n",
    "            print(\"Erro: Coluna 'customer_zip_code_prefix' ausente no DataFrame 'customers'.\")\n",
    "            dfs_disponiveis = False\n",
    "        if not all(col in geolocation_agg_df.columns for col in ['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']):\n",
    "            print(\"Erro: Colunas 'geolocation_zip_code_prefix', 'geolocation_lat' ou 'geolocation_lng' ausentes no DataFrame 'geolocation_agg'.\")\n",
    "            dfs_disponiveis = False\n",
    "\n",
    "        if dfs_disponiveis:\n",
    "            # Juntar customers com geolocation_agg para obter lat/lng\n",
    "            merged_geo_customers = pd.merge(\n",
    "                customers_df[['customer_id', 'customer_zip_code_prefix']],\n",
    "                geolocation_agg_df[['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']],\n",
    "                left_on='customer_zip_code_prefix',\n",
    "                right_on='geolocation_zip_code_prefix',\n",
    "                how='inner'\n",
    "            )\n",
    "            merged_geo_customers.dropna(subset=['geolocation_lat', 'geolocation_lng'], inplace=True)\n",
    "\n",
    "            if not merged_geo_customers.empty:\n",
    "                locations_data = merged_geo_customers[['geolocation_lat', 'geolocation_lng']].values.tolist()\n",
    "                print(f\"Dados de localização de {len(locations_data)} clientes preparados.\")\n",
    "            else:\n",
    "                print(\"Nenhum dado de localização de cliente encontrado após o merge. Verifique os dados de CEP e geolocalização.\")\n",
    "                dfs_disponiveis = False\n",
    "        else:\n",
    "             print(\"Não foi possível prosseguir com dados reais devido a colunas ausentes.\")\n",
    "\n",
    "\n",
    "    if not dfs_disponiveis or locations_data is None or len(locations_data) == 0:\n",
    "        print(\"Não é possível gerar o mapa de densidade com dados reais. Simulando pontos no Brasil...\")\n",
    "        min_lat, max_lat = -33.75, 5.27\n",
    "        min_lon, max_lon = -73.98, -34.79\n",
    "        np.random.seed(42)\n",
    "        sim_lat = np.random.uniform(min_lat, max_lat)\n",
    "        sim_lon = np.random.uniform(min_lon, max_lon)\n",
    "        locations_data = [[lat, lon] for lat, lon in zip(sim_lat, sim_lon)]\n",
    "\n",
    "\n",
    "    if locations_data is None or len(locations_data) == 0:\n",
    "        print(\"Nenhum dado de localização (real ou simulado) disponível. Não é possível gerar o mapa.\")\n",
    "        return\n",
    "\n",
    "    map_center_lat = -14.2350\n",
    "    map_center_lon = -51.9253\n",
    "    brazil_map = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=4)\n",
    "\n",
    "    HeatMap(locations_data, radius=10, blur=10).add_to(brazil_map)\n",
    "\n",
    "    print(\"Gerando o mapa de densidade...\")\n",
    "    display(brazil_map)\n",
    "    print(\"Mapa de densidade de clientes gerado e exibido.\")\n",
    "\n",
    "\n",
    "gerar_mapa_densidade_clientes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE-P2jgdHmQu"
   },
   "source": [
    "# Impacto do Tempo de Entrega na Avaliação do Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WK4Bg-qZHmlQ",
    "outputId": "2049bac0-d55f-4d92-a64c-df9270495f25"
   },
   "outputs": [],
   "source": [
    "def visualizar_impacto_tempo_entrega_na_avaliacao():\n",
    "    \"\"\"\n",
    "    Gera gráficos para visualizar como o tempo de entrega impacta a avaliação dos clientes.\n",
    "    1. Gráfico de Barras: Avaliação média por faixas de tempo de entrega.\n",
    "    2. Boxplot: Distribuição do tempo de entrega para cada nota de avaliação.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Visualizando Impacto do Tempo de Entrega na Avaliação do Cliente ---\")\n",
    "\n",
    "    if 'orders' not in globals() or 'order_reviews' not in globals():\n",
    "        print(\"DataFrames 'orders' ou 'order_reviews' não encontrados globalmente.\")\n",
    "        print(\"Por favor, carregue e prepare os dados em células anteriores antes de executar esta.\")\n",
    "        return\n",
    "\n",
    "    reviews_df = globals()['order_reviews'][['order_id', 'review_score']].copy()\n",
    "    orders_df = globals()['orders'][['order_id', 'order_purchase_timestamp', 'order_delivered_customer_date']].copy()\n",
    "\n",
    "    df_viz = pd.merge(reviews_df, orders_df, on='order_id', how='left')\n",
    "\n",
    "    df_viz['order_purchase_timestamp'] = pd.to_datetime(df_viz['order_purchase_timestamp'])\n",
    "    df_viz['order_delivered_customer_date'] = pd.to_datetime(df_viz['order_delivered_customer_date'])\n",
    "    df_viz['tempo_entrega_dias'] = (df_viz['order_delivered_customer_date'] - df_viz['order_purchase_timestamp']).dt.days\n",
    "\n",
    "    df_viz.dropna(subset=['review_score', 'tempo_entrega_dias'], inplace=True)\n",
    "    df_viz = df_viz[(df_viz['tempo_entrega_dias'] >= 0) & (df_viz['tempo_entrega_dias'] <= 90)]\n",
    "\n",
    "    if df_viz.empty:\n",
    "        print(\"Nenhum dado disponível após a preparação para visualização do impacto do tempo de entrega.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 16))\n",
    "    fig.suptitle('Impacto do Tempo de Entrega na Avaliação do Cliente', fontsize=18, y=1.03)\n",
    "\n",
    "    max_days_for_bins = int(df_viz['tempo_entrega_dias'].max())\n",
    "    if max_days_for_bins == 0:\n",
    "        max_days_for_bins = 1\n",
    "\n",
    "    if max_days_for_bins <= 10:\n",
    "        bins = list(range(0, max_days_for_bins + 2, 1))\n",
    "        labels = [f'{i} dia(s)' for i in range(len(bins)-1)]\n",
    "    elif max_days_for_bins <= 30:\n",
    "        bins = list(range(0, max_days_for_bins + 4, 3))\n",
    "        labels = [f'{bins[i]}-{bins[i+1]-1} dias' for i in range(len(bins)-1)]\n",
    "    else:\n",
    "        bins = [0, 8, 16, 31, 46, 61, max_days_for_bins + 1]\n",
    "        labels = ['0-7 dias', '8-15 dias', '16-30 dias', '31-45 dias', '46-60 dias', f'61-{max_days_for_bins} dias']\n",
    "\n",
    "    bins = sorted(list(set(bins)))\n",
    "    if bins[-1] <= max_days_for_bins:\n",
    "        bins[-1] = max_days_for_bins + 1\n",
    "\n",
    "    current_labels = []\n",
    "    for i in range(len(bins)-1):\n",
    "        if bins[i+1] == bins[i]+1:\n",
    "             current_labels.append(f'{bins[i]} dia(s)')\n",
    "        else:\n",
    "             current_labels.append(f'{bins[i]}-{bins[i+1]-1} dias')\n",
    "    labels = current_labels\n",
    "\n",
    "    df_viz['faixa_tempo_entrega'] = pd.cut(df_viz['tempo_entrega_dias'],\n",
    "                                           bins=bins,\n",
    "                                           labels=labels,\n",
    "                                           right=False,\n",
    "                                           include_lowest=True)\n",
    "\n",
    "    avg_score_by_delivery_time = df_viz.groupby('faixa_tempo_entrega', observed=False)['review_score'].mean().reset_index()\n",
    "\n",
    "    sns.barplot(x='faixa_tempo_entrega', y='review_score', data=avg_score_by_delivery_time, ax=axes[0], palette=\"coolwarm\", hue='faixa_tempo_entrega', dodge=False, legend=False)\n",
    "    axes[0].set_title('Avaliação Média do Cliente por Faixa de Tempo de Entrega', fontsize=14)\n",
    "    axes[0].set_xlabel('Tempo de Entrega', fontsize=12)\n",
    "    axes[0].set_ylabel('Avaliação Média (1-5)', fontsize=12)\n",
    "    axes[0].set_ylim(0, 5.5)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    plt.setp(axes[0].get_xticklabels(), ha='right', rotation_mode='anchor') # Alinhar os rótulos\n",
    "\n",
    "    for index, row in avg_score_by_delivery_time.iterrows():\n",
    "        if pd.notnull(row.review_score):\n",
    "            axes[0].text(index, row.review_score + 0.1, f'{row.review_score:.2f}', color='black', ha=\"center\", fontsize=9)\n",
    "\n",
    "    sns.boxplot(x='review_score', y='tempo_entrega_dias', data=df_viz, ax=axes[1], palette=\"viridis\", hue='review_score', legend=False)\n",
    "    axes[1].set_title('Distribuição do Tempo de Entrega por Nota de Avaliação do Cliente', fontsize=14)\n",
    "    axes[1].set_xlabel('Avaliação do Cliente (1-5)', fontsize=12)\n",
    "    axes[1].set_ylabel('Tempo de Entrega (dias)', fontsize=12)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Visualizações do impacto do tempo de entrega na avaliação geradas.\")\n",
    "\n",
    "# Chamada da função para gerar os gráficos.\n",
    "visualizar_impacto_tempo_entrega_na_avaliacao()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRXFOMmUHm-3"
   },
   "source": [
    "# Dashboard de Análise de Vendedores\n",
    "Um dashboard de análise dos vendedores, mostrando quais têm melhor desempenho em termos de volume de vendas, satisfação do cliente e tempo de entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RqjXFFbEHnRr",
    "outputId": "a45d40d5-15f3-43d3-98af-38f5e2f92746"
   },
   "outputs": [],
   "source": [
    "def dashboard_analise_vendedores():\n",
    "    \"\"\"\n",
    "    Gera um dashboard de análise de desempenho dos vendedores, mostrando\n",
    "    volume de vendas, satisfação do cliente e tempo de entrega.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Dashboard de Análise de Vendedores ---\")\n",
    "\n",
    "    required_dfs = ['order_items', 'orders', 'order_reviews', 'sellers']\n",
    "    if not all(df_name in globals() for df_name in required_dfs):\n",
    "        print(f\"DataFrames necessários ({', '.join(required_dfs)}) não encontrados globalmente.\")\n",
    "        print(\"Por favor, carregue e prepare os dados em células anteriores antes de executar esta.\")\n",
    "        return\n",
    "\n",
    "    # Usa cópias para evitar modificar os DataFrames globais\n",
    "    df_items = globals()['order_items'].copy()\n",
    "    df_orders = globals()['orders'].copy()\n",
    "    df_reviews = globals()['order_reviews'].copy()\n",
    "    df_sellers = globals()['sellers'][['seller_id', 'seller_city', 'seller_state']].copy()\n",
    "\n",
    "    # 1. Preparar dados base\n",
    "    df_merged = pd.merge(df_items[['order_id', 'order_item_id', 'product_id', 'seller_id', 'price', 'freight_value']],\n",
    "                         df_orders[['order_id', 'customer_id', 'order_purchase_timestamp',\n",
    "                                    'order_delivered_customer_date', 'order_status']],\n",
    "                         on='order_id', how='inner')\n",
    "\n",
    "    df_merged = pd.merge(df_merged,\n",
    "                         df_reviews[['order_id', 'review_score']],\n",
    "                         on='order_id', how='left')\n",
    "\n",
    "    df_merged['order_purchase_timestamp'] = pd.to_datetime(df_merged['order_purchase_timestamp'])\n",
    "    df_merged['order_delivered_customer_date'] = pd.to_datetime(df_merged['order_delivered_customer_date'])\n",
    "    df_merged['tempo_entrega_dias'] = (df_merged['order_delivered_customer_date'] - df_merged['order_purchase_timestamp']).dt.days\n",
    "\n",
    "    df_entregues = df_merged[df_merged['order_status'] == 'delivered'].copy()\n",
    "    df_entregues.dropna(subset=['tempo_entrega_dias'], inplace=True)\n",
    "    df_entregues = df_entregues[df_entregues['tempo_entrega_dias'] >= 0]\n",
    "\n",
    "    # 2. Calcular métricas por vendedor\n",
    "    vendas_por_vendedor = df_merged.groupby('seller_id').agg(\n",
    "        total_vendas_valor=('price', 'sum'),\n",
    "        total_itens_vendidos=('order_item_id', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    satisfacao_por_vendedor = df_entregues.dropna(subset=['review_score']).groupby('seller_id').agg(\n",
    "        media_review_score=('review_score', 'mean'),\n",
    "        num_reviews=('review_score', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    entrega_por_vendedor = df_entregues.groupby('seller_id').agg(\n",
    "        tempo_medio_entrega_dias=('tempo_entrega_dias', 'mean'),\n",
    "        num_entregas=('order_id', 'nunique')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_vendedor_performance = pd.merge(vendas_por_vendedor, satisfacao_por_vendedor, on='seller_id', how='left')\n",
    "    df_vendedor_performance = pd.merge(df_vendedor_performance, entrega_por_vendedor, on='seller_id', how='left')\n",
    "    df_vendedor_performance = pd.merge(df_vendedor_performance, df_sellers, on='seller_id', how='left')\n",
    "\n",
    "    if df_vendedor_performance.empty:\n",
    "        print(\"Não foi possível gerar dados de performance dos vendedores.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nDataFrame de Performance dos Vendedores (Top 5 por Vendas):\")\n",
    "    display(df_vendedor_performance.sort_values(by='total_vendas_valor', ascending=False).head())\n",
    "\n",
    "    # 3. Visualizações\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 20)) # Ajustado figsize para melhor visualização no Colab\n",
    "    fig.suptitle('Dashboard de Análise de Desempenho dos Vendedores', fontsize=18, y=1.01)\n",
    "\n",
    "    top_n = 10\n",
    "    min_reviews_threshold = 10\n",
    "    min_entregas_threshold = 10\n",
    "\n",
    "    # Gráfico 1: Top N Vendedores por Volume de Vendas\n",
    "    top_vendedores_volume = df_vendedor_performance.sort_values(by='total_vendas_valor', ascending=False).head(top_n)\n",
    "    sns.barplot(x='total_vendas_valor', y='seller_id', data=top_vendedores_volume, ax=axes[0], palette='viridis', hue='seller_id', dodge=False, legend=False)\n",
    "    axes[0].set_title(f'Top {top_n} Vendedores por Volume de Vendas (R$)', fontsize=15)\n",
    "    axes[0].set_xlabel('Volume de Vendas (R$)', fontsize=13)\n",
    "    axes[0].set_ylabel('ID do Vendedor', fontsize=13)\n",
    "    axes[0].tick_params(axis='y', rotation=0)\n",
    "    for i, p in enumerate(axes[0].patches):\n",
    "        value = p.get_width()\n",
    "        axes[0].text(value + (top_vendedores_volume['total_vendas_valor'].max() * 0.01),\n",
    "                     p.get_y() + p.get_height() / 2,\n",
    "                     f'R$ {value:,.0f}', va='center', fontsize=10)\n",
    "\n",
    "    # Gráfico 2: Top N Vendedores por Satisfação Média do Cliente\n",
    "    vendedores_com_reviews_suficientes = df_vendedor_performance[df_vendedor_performance['num_reviews'] >= min_reviews_threshold]\n",
    "    if not vendedores_com_reviews_suficientes.empty:\n",
    "        top_vendedores_satisfacao = vendedores_com_reviews_suficientes.sort_values(by='media_review_score', ascending=False).head(top_n)\n",
    "        sns.barplot(x='media_review_score', y='seller_id', data=top_vendedores_satisfacao, ax=axes[1], palette='coolwarm', hue='seller_id', dodge=False, legend=False)\n",
    "        axes[1].set_title(f'Top {top_n} Vendedores por Satisfação Média (Mín. {min_reviews_threshold} Reviews)', fontsize=15)\n",
    "        axes[1].set_xlabel('Satisfação Média (1-5)', fontsize=13)\n",
    "        axes[1].set_ylabel('ID do Vendedor', fontsize=13)\n",
    "        axes[1].set_xlim(0, 5.5)\n",
    "        axes[1].tick_params(axis='y', rotation=0)\n",
    "        for i, p in enumerate(axes[1].patches):\n",
    "            value = p.get_width()\n",
    "            axes[1].text(value + 0.05, p.get_y() + p.get_height() / 2, f'{value:.2f}', va='center', fontsize=10)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, f'Não há vendedores suficientes com >={min_reviews_threshold} reviews.', ha='center', va='center', fontsize=12, transform=axes[1].transAxes)\n",
    "        axes[1].set_title(f'Top {top_n} Vendedores por Satisfação Média', fontsize=15)\n",
    "\n",
    "    # Gráfico 3: Top N Vendedores por Menor Tempo Médio de Entrega\n",
    "    vendedores_com_entregas_suficientes = df_vendedor_performance[df_vendedor_performance['num_entregas'] >= min_entregas_threshold]\n",
    "    if not vendedores_com_entregas_suficientes.empty:\n",
    "        top_vendedores_entrega = vendedores_com_entregas_suficientes.sort_values(by='tempo_medio_entrega_dias', ascending=True).head(top_n)\n",
    "        sns.barplot(x='tempo_medio_entrega_dias', y='seller_id', data=top_vendedores_entrega, ax=axes[2], palette='mako', hue='seller_id', dodge=False, legend=False)\n",
    "        axes[2].set_title(f'Top {top_n} Vendedores por Menor Tempo Médio de Entrega (Mín. {min_entregas_threshold} Entregas)', fontsize=15)\n",
    "        axes[2].set_xlabel('Tempo Médio de Entrega (dias)', fontsize=13)\n",
    "        axes[2].set_ylabel('ID do Vendedor', fontsize=13)\n",
    "        axes[2].tick_params(axis='y', rotation=0)\n",
    "        for i, p in enumerate(axes[2].patches):\n",
    "            value = p.get_width()\n",
    "            axes[2].text(value + (top_vendedores_entrega['tempo_medio_entrega_dias'].max() * 0.01 if not top_vendedores_entrega.empty else 0.1),\n",
    "                         p.get_y() + p.get_height() / 2,\n",
    "                         f'{value:.1f} dias', va='center', fontsize=10)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, f'Não há vendedores suficientes com >={min_entregas_threshold} entregas.', ha='center', va='center', fontsize=12, transform=axes[2].transAxes)\n",
    "        axes[2].set_title(f'Top {top_n} Vendedores por Menor Tempo Médio de Entrega', fontsize=15)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Dashboard de análise de vendedores gerado.\")\n",
    "\n",
    "dashboard_analise_vendedores()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
